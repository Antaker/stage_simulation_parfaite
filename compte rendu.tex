
\documentclass[a4paper,oneside,11pt]{article}



%% Language %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage[french]{babel} %francais, polish, spanish, ...
\usepackage[T1]{fontenc}
\usepackage[ansinew]{inputenc}
\usepackage{lmodern} %Type1-font for non-english texts and characters
\usepackage{xcolor}
\usepackage{amsmath,amsfonts,amssymb}


%% Packages for Graphics & Figures %%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{graphicx} %%For loading graphic files
\usepackage{caption} 


%% Math Packages %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amsfonts}
\newtheorem{definition}{Définition}
\newtheorem{theorem}{Th\'eor\`eme}
\newtheorem{lemme}{Lemme}

%%inclusion de code et pseudo-code%%
\usepackage{listings} %%permet l'insertion de code dans le texte%%
\usepackage{algorithm}
\usepackage{algorithmic} 

\makeatletter
\renewcommand{\fnum@algorithm}{\fname@algorithm}
\makeatother


\lstset{
language=R,
basicstyle=\ttfamily\small, %
identifierstyle=\color{red}, %
keywordstyle=\color{blue}, %
stringstyle=\color{black!60}, %
columns=flexible, %
tabsize=2, %
extendedchars=true, %
showspaces=false, %
showstringspaces=false, %
numbers=left, %
numberstyle=\tiny, %
breaklines=true, %
breakautoindent=true, %
captionpos=b, %
rulecolor=\color{black}
}



\begin{document}


\pagestyle{empty} %No headings for the first pages.



\title{Compte rendu stage \\
Simulation parfaite}
\author{Antonin LAURENT}
%\date{} %%If commented, the current date is used.
\maketitle

\newpage

%% Inhaltsverzeichnis %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\tableofcontents %Table of contents

\pagestyle{plain} %Now display headings: headings / fancy / ...

\newpage



\section{Chapitre 1 : Introduction}

\subsection{Définitions et théorèmes}

\begin{definition}
Temps d'arrêt
\end{definition}

On dit que $T$ est un temps d'arrêt pour une suite $X_{1},X_{2},\ldots$ si, connaissant les valeurs de $X_{1},X_{2},\ldots,X_{n}$ on peut déterminer si $T \leq n$.

\begin{definition} 
Fonction calculable
\end{definition}

Une fonction est dite calculable (computable en anglais) si il existe un algorithme capable de retourner le résultat de la fonction.

\begin{definition}
Algorithme probabiliste
\end{definition}

Soient 	$\cal{I}$ un ensemble d'indices tel que pour tout $I \in \cal{I}$, il existe une distribution $\pi_{I}$ sur un espace d'états $\Omega_{I}$. On se donne une suite de variables aléatoires $X_{1},X_{2},\ldots$ où les $X_{I} \in S_{I}$ .\\
Un algorithme probabiliste est une famille de temps d'arrêts $ \left\{ T_{I} \right\}_{I \in \cal{I}}$ et et de fonctions calculables 
$ \left\{ f_{I,t} \right\} _{i \in \mathcal{I}, t \in \left\{ 1,2,\ldots \right\} } $. La sortie de l'algorithme est $f_{I,T_{I}}(X_{1},\ldots,X_{T_{I}})$.\\

\begin{definition}
Algorithme de simulation parfaite
\end{definition}

Un algorithme de simulation parfaite est un algorithme probabiliste dont la sortie est une variable aléatoire qui provient d'une distribution cible.\\
Les algorithmes de simulation parfaite sont une sous-classe des algorithmes de simulation exacte, algorithmes qui tirent d'une distribution ciblée. \\
Cependant les algorithmes dont le temps d'arrêt 
$T_{I}$ est déterministe (algorithmes tournant selon un nombre fini de choix aléatoires) sont généralement considérés comme algorithmes de simulation exacte mais pas comme algorithmes de simulation parfaite.


\begin{theorem}[Théorème fondamental de la simulation parfaite]
On suppose que pour $U_{1},U_{2},\ldots$ iid tel que $U_{i} \sim$ Unif([0,1]) , il existe des fonctions calculables $b,g$ et $f$ telles que la fonction $b$ aie pour image $ \left\{ 0,1 \right\} $ et $\mathbb{P}(b(U) = 1) > 0$. \\
Pour une variable aléatoire X qui vérifie : 

\begin{equation}
 X \sim b(U)g(U) + (1 - b(U))f(X,U),
 \label{eq1}
\end{equation}




soit $T =$ inf$\left\{t : b(U_{t}) = 1\right\}$. On a alors que :

$$ Y = f(\ldots f(f(g(U_{T}),U_{T-1}),U_{T-2}),\ldots,U_{1})$$

a la même distribution que $X$ et on a $\mathbb{E}[T] = \frac{1}{\mathbb{P}(b(U) = 1)}$.
\end{theorem}

\begin{proof}
Soient $X_{0},X_{1},\ldots$ des tirages indépendants chacun distribués selon $X$, et $U_{1},U_{2},\ldots$ iid tel que $U_{i} \sim$ Unif([0,1]). Pour $X_{t}$, fixons $X_{t,t} = X_{t}$ et récursivement, on pose :

$$ X_{t,i} = b(U_{i+1})g(U_{i+1}) + (1 - b(U_{i+1}))f(X_{t,i+1},U_{i+1}),  $$
pour $i \in \left\{0,\ldots,t-1\right\}$.\\

On a alors d'après la relation \eqref{eq1} : $X_{t,0} \sim X$. On montre ce résultat pour les premiers indices t, les suivants sont prouvés de la même manière.
\paragraph{\underline{t=0}}

\begin{align*}
X_{0,0} = X_{0} \\
\intertext{comme on l'a posé précédemment, d'où} 
X_{0,0} \sim X.
\end{align*}

\paragraph{\underline{t=1}}

\begin{align*}
X_{1,0} = b(U_{1})g(U_{1}) + (1 - b(U_{1}))f(X_{1,1},U_{1})\\ 
\end{align*}

Or, 

\begin{align*}
X_{1,1} = X_{1} \sim X.\\
\end{align*}

En remplaçant dans l'équation précédente, on obtient :

\begin{align*}
X_{1,0} = b(U_{1})g(U_{1}) + (1 - b(U_{1}))f(X_{1},U_{1})\\ 
\end{align*}
\qquad D'après la relation \eqref{eq1}, on obtient alors $X_{1,0} = X_{1} \sim X$.\\

\paragraph{\underline{t=2}}

\begin{align*}
X_{2,0} &= b(U_{1})g(U_{1}) + (1 - b(U_{1}))f(X_{2,1},U_{1})\\ 
X_{2,1} &= b(U_{2})g(U_{2}) + (1 - b(U_{2}))f(X_{2,2},U_{2})\\
&= b(U_{2})g(U_{2}) + (1 - b(U_{2}))f(X_{2},U_{2})\\
\end{align*}
À nouveau, par la relation \eqref{eq1}, on a : $X_{2,1} = X_{2}$
On remplace donc par cette valeur dans $X_{2,0}$ et on obtient :

\begin{align*}
X_{2,0} &= b(U_{1})g(U_{1}) + (1 - b(U_{1}))f(X_{2},U_{1})\\
\end{align*}

D'après la relation \eqref{eq1}, on obtient alors $X_{2,0} = X_{2} \sim X$.\\

On a donc $X_{0,0},X_{1,0},X_{2,0},\ldots$ de même distribution que $X$ mais pas forcément indépendants. On considère à présent la variable $Y$ énoncée dans le théorème. On a alors la relation suivante : $X_{t,0} = Y$ si $t \geq T$. On illustre ce résultat avec comme exemple $T=2$ et $t=2,3$ : \\
On a donc : 
$$ Y = f(g(U_{2}),U_{1}) $$

Montrons que $X_{2,0} = X_{3,0} = Y$. Étant donné que $T=2$, on a $b(U_{1}) = 0$ et $b(U_{2}) = 1$ , d'où :

\begin{align*}
X_{2,0} &= f(X_{2,1},U_{1})\\
X_{2,1} &= g(U_{2})\\
\intertext{on remplace dans la première équation et on obtient}
X_{2,0} &= f(g(U_{2}),U_{1})\\
\end{align*}

D'où le résultat pour t=2. Voyons pour t=3.\\
~\\
De même, puisque $T=2$, on a $b(U_{1}) = 0$ et $b(U_{2}) = 1$ , d'où :

\begin{align*}
X_{3,0} &= f(X_{3,1},U_{1})\\
X_{3,1} &= g(U_{2})\\
\intertext{on remplace dans la première équation et on obtient}
X_{3,0} &= f(g(U_{2}),U_{1})\\
\end{align*}

D'où le résultat.\\
Ensuite, puisque les $U_{i}$ sont indépendants, on a que : $\mathbb{P}(T > t) = (1 - \mathbb{P}(b(U) = 1))^{t}$ et puisque, par hypothèse, $\mathbb{P}(b(U) = 1) >0$, on a la relation qui tend vers $0$ pour $t$ tendant vers l'infini.
Il ne reste plus qu'à montrer $Y \sim X$. Pour tout $t$, pour tout ensemble $C$ :

\begin{align*}
\mathbb{P}(Y \in C) &= \mathbb{P}(Y \in C, t \geq T) + \mathbb{P}(Y \in C, t < T)
\intertext{puisque $X_{t,0} = Y$ si $t \geq T$, on a}
&= \mathbb{P}(X_{t,0} \in C, t \geq T) + \mathbb{P}(Y \in C, t < T)\\
&= \mathbb{P}(X_{t,0} \in C) - \mathbb{P}(X_{t,0} \in C, t < T) + \mathbb{P}(Y \in C, t < T)
\intertext{puisque $X_{t,0} \sim X$, on a}
&= \mathbb{P}(X \in C) - \mathbb{P}(X_{t,0} \in C, t < T) + \mathbb{P}(Y \in C, t < T)
\end{align*}

Les deux derniers termes sont bornés par $\mathbb{P}(T > t) = (1 - \mathbb{P}(b(U) = 1))^{t}$, et puisque l'équation est vraie pour n'importe quel $t$,  on obtient : $\mathbb{P}(Y \in C) = \mathbb{P}(X \in C)$ pour tout ensemble $C$ , donc $Y \sim X$.
Le fait que $\mathbb{E}[T] = \frac{1}{\mathbb{P}(b(U)=1)}$ provient du fait que $T$ suit une loi géométrique de paramètre $\mathbb{P}(b(U) =1)$

\end{proof}




\begin{definition}
Algorithme interruptible
\end{definition}
Dans le théorème fondamental de la simulation parfaite, si $X$ et $T$ sont indépendants, on dit que l'algorithme est interruptible, sinon il est non-interruptible. 

\begin{definition}
Algortihme de simulation parfaite à lecture unique
\end{definition}
Un algorithme de simulation parfaite qui utilise $X \sim b(U)g(U) + (1 - b(U))f(X,U)$ est un algorithme à lecture unique si $f(X,u) = f(X,u')$ pour tout $u$,$u'$. Sinon, c'est un algorithme à lecture double.\\
~\\
En général, un algorithme interruptible est préférable à un algorithme non-interruptible, et un algorithme à lecture unique est préférable à un algorithme à lecture double.\\

\subsection{Modèles étudiés}

\subsubsection{Champs aléatoires de Markov}

On considère un graphe $G = (V,E)$. $V$ est l'ensemble des n\oe{}uds et $E$ est l'ensemble des arêtes. On notera par la suite $\Delta$ le degré du graphe (égal au degré du n\oe{}ud ayant un nombre de voisins maximal). On notera par $\Omega$ l'ensemble des labels des n\oe{}uds.

\begin{definition}
Ensemble séparant
\end{definition}
On dit qu'un sous-ensemble de n\oe{}uds $S$ sépare les n\oe{}uds $i$ et $j$ si tout chemin du graphe menant $i$ à $j$ passe par $S$.

\begin{definition}
Champ aléatoire de Markov
\end{definition}
Pour un graphe $G = (V,E)$ et un ensemble de labels $\Omega$, on dit que la distribution de $X$ sur $\Omega$ est un champ aléatoire de Markov si, pour tous les n\oe{}uds $i$ et $j$, pour tous les ensembles $S$ séparant $i$ de $j$, on a : $[X(i)|X(j),X(S)] \sim [X(i)|X(S)]$. Un état $x \in \Omega$ est appelé une configuration.

\begin{definition}
Clique
\end{definition}
Une clique est un sous-ensemble de n\oe{}uds du graphe tel que chaque paire de n\oe{}uds soit connectée par une arête.


\begin{theorem}[Théorème de Hammersley-Clifford]
Pour un graphe fini $G=(V,E)$, la distribution $\pi$ est un champ aléatoire de Markov si elle a pour densité $f_{X}$ et qu'il existe des fonctions $\phi _{C}$ pour toute clique C telles que $f_{X}$ puisse s'écrire : 

$$ f_{X}(x) = \frac{1}{Z} \prod _{C \in cliques(G)} \phi_{C}(x)$$
\end{theorem}



\begin{definition}
Auto-modèle
\end{definition}
On dit qu'un champ aléatoire de Markov est un auto-modèle, si il existe des fonctions $f_{i}$ et $g_{i}$ telles que la densité de $X \sim \pi$ peut-être écrite sous la forme : 
$$ f_{X}(x) = \frac{1}{Z} \left[\prod_{i \in V} f_{i}(X(i))\right]\left[ \prod _{ \left\{i,j \right\} \in E} g_{\left\{i,j\right\}}(X(i),X(j))\right]$$

Dans la suite, on définit des exemples bien connus d'auto-modèles que l'on étudiera par la suite.

\paragraph{Exemple 1 : Modèle d'Ising}
~\\
~\\
Le modèle d'Ising est un auto-modèle de paramètres $\mu$ et $\beta$ , où $\Omega = \left\{ -1,1 \right\} ^{V}$, et $f(c) = exp(\mu c)$, $g(c_{1},c_{2}) = exp(\beta \mathbf{1}(c_{1} = c_{2}))$. Dans la littérature, on appelle $\mu$ magnétisation et $\beta$ la température inverse. Lorsque $\beta >0$, on dit que le modèle est ferromagnétique. Lorsque $\mu = 0$, on dit que le modèle est sans champ magnétique.

\paragraph{Exemple 2 : Modèle Hard-core gas}
~\\
~\\ 
Le modèle hard-core gas est un auto-modèle défini sur $\left\{0,1 \right\} ^{V}$ et de paramètre $\lambda >0$ où $f(c) = \lambda ^{c}$ 
et $g(c_{1},c_{2}) = 1 - c_{1}c_{2}$. Lorsqu'un no\oe{}ud $\nu$ a pour label $1$, on dit que le n\oe{}ud est occupé, sinon, il est inoccupé.

\paragraph{Exemple 3 : Modèle de Strauss}
~\\
~\\
Le modèle de Strauss est un auto-modèle sur $\left\{0,1 \right\} ^{V}$ de paramètres $\lambda >0$ et $\gamma \in [0,1]$ où $f(c) = \lambda ^{c}$ et $g(c_{1},c_{2}) = 1 + (\gamma - 1)c_{1}c_{2}$.Lorsqu'un no\oe{}ud $\nu$ a pour label $1$, on dit que le n\oe{}ud est occupé, sinon, il est inoccupé.

\subsubsection{Permutations}

Un problème classique de distribution sur les permutations est lié à la recherche du permanent d'une matrice non-négative.

\paragraph{Exemple}
~\\
~\\
Pour une matrice non-négative $w(i,j)$, notons :

$$w(\sigma) = \prod_{i=1}^{n} w(i,\sigma(i))$$
\\
\nolinebreak Tant qu'il existe au moins une permutation $\sigma$ telle que $w(i,\sigma(i)) >0$ pour tout $i$, cela donne une densité non normalisée sur l'ensemble des permutations. La constante de normalisation pour cette densité est alors appelée le permanent de la matrice $w(i,j)$. Si aucun $\sigma$ ne vérifie cette relation, le permanent est 0.
~\\
Une autre distribution importante que l'on va considérer est la distribution uniforme sur les permutations où certains objets doivent avoir une position plus faible que les autres.

\begin{definition}
Relation d'ordre partiel
\end{definition}
Soit un ensemble $P$. Une relation d'ordre partiel sur $P$ est une relation binaire $\preceq$ telle que pour tout $a,b,c \in P$, la relation est:

\begin{enumerate}
\item (Réflexive) $a \preceq a$
\item (Antisymétrique) Si $a \preceq b$ et $b \preceq a$, alors $a = b$
\item (Transitive) Si $a \preceq b$ et $b \preceq c$, alors $a \preceq c$
\end{enumerate}

Un ensemble disposant d'une relation d'ordre partiel est parfois appelé poset d'après l'anglais partially ordered set.

\begin{definition}
Extension linéaire d'un ordre partiel
\end{definition}

Une extension linéaire d'un ordre partiel sur $1,\ldots,n$ est une permutation pour laquelle si $i$ et $j$ sont tels que $\sigma(i) \prec \sigma(j)$, alors $i<j$.  

\subsection{Chaînes de Markov et simulation approchée}

Jusqu'au développement des algorithmes de simulation parfaite/exacte, la manière principale d'obtenir une réalisation d'une distribution ciblée était une méthode d'approche. Plusieurs algorithmes et méthodes ont été mis en place et l'ensemble de ces méthodes porte le nom de Chaîne de Markov Monte Carlo (Markov Chain Monte Carlo, MCMC dans la littérature).
~\\
On ne rappellera pas ici les principales définitions pour les chaînes de Markov mais d'autres seront nécessaires pour la suite.
On s'intéressera notamment aux chaînes de Harris et au théorème ergodique associé.

\begin{definition}
Chaîne de Harris
\end{definition}

Une chaîne de Markov $\left\{X_{t} \right\}$ sur un espace d'état $\Omega$ est une chaîne de Harris si il existe des ensembles mesurables $A,B \in \Omega$ et $\epsilon > 0$ pour $x \in A$ et $y \in B$, et une mesure de probabilité $\rho$ où $\rho(B) = 1$ tels que l'on ait :

\begin{enumerate}
\item Pour $T_{A} = inf\left\{ t \geq 0 : X_{t} \in A \right\}, ( \forall z \in \Omega)(\mathbb{P}(T_{A} < \infty | X_{0} = z) > 0).$
\item Si $x \in A$ et $C \subseteq B$, alors $\mathbb{P}(X_{1} \in C | X_{0} = x) \geq \epsilon \rho (C)$
\end{enumerate}  

\begin{definition}
Chaîne récurrente
\end{definition}

Soit $R = \inf \left\{ n>0 : X_{n} \in A \right\}$. On dit qu'une chaîne de Harris est une chaîne récurrente si pour tout $x \in A$, $\mathbb{P}(R < \infty | X_{0} = x) = 1$. Une chaîne qui n'est pas récurrente est dite transiente.

\begin{definition}
Chaîne apériodique
\end{definition}

Une chaîne de Harris récurrente est apériodique si pour tout $x \in \Omega$, il existe $n$ tel que pour tout $n' \geq n$, $\mathbb{P}(X_{n'} \in A | X_{0} = x) >0$

\begin{theorem}[Théorème ergodique pour les chaînes de Harris]
Soit $X_{n}$ une chaîne de Harris récurrente et apériodique de distribution stationnaire $\pi$. Si $\mathbb{P}(R < \infty | X_{0} = x) = 1$ pour tout $x$, alors, pour $t \to \infty$, pour tout ensemble mesurable $C$ et pour tout $x$ : 

$$ |\mathbb{P}(X_{t} \in C|X_{0} = x) - \pi(C)| \to 0 $$
\end{theorem} 

Ce théorème est le c\oe{}ur des méthodes MCMC, puisqu'il "suffit" de construire une chaîne de Harris ayant pour distribution stationnaire la distribution souhaitée et de la faire tourner pendant un nombre infini de pas. Cependant, n'ayant pas un temps infini, les utilisateurs font tourner leurs algorithmes durant un grand nombre de pas et espèrent arriver dans la distribution stationnaire.
~\\
\linebreak Nous pourrons cependant déterminer à quel point la chaîne est proche de la loi stationnaire à l'aide du concept de couplage (voir chapitre 3 pour une définition et application étendue).

\begin{definition}
Couplage
\end{definition}
Supposons que $\left\{X_{t} \right\} \sim \nu_{X}$ et $\left\{Y_{t} \right\} \sim \nu_{Y}$. Un couplage de $\left\{X_{t} \right\}$ et $\left\{Y_{t} \right\}$ est un processus bivarié $\left\{(X^{'}_{t},Y^{'}_{t})\right\}$ tel que $\left\{X^{'}_{t} \right\} \sim \nu_{X}$ et $\left\{Y^{'}_{t} \right\} \sim \nu_{Y}$.

\begin{theorem}[Lemme de Couplage]
Soit $Y_{0} \sim \pi$ et $X_{0} = x_{0}$ tels que les deux variables évoluent de manière couplée. Alors, pour tout mesurable $C$ :
$$ |\mathbb{P}(X_{t} \in C|X_{0}=x) - \pi(C)| \leq \mathbb{P}(X_{t} \ne Y_{t}).$$
\end{theorem} 



\subsection{Création de chaînes de Markov}

Afin d'utiliser les méthodes MCMC, il faut créer des chaînes de Harris qui convergent vers la distribution $\pi$ ciblée.
Il est généralement mieux de créer des chaînes réversibles plutôt que des chaînes simplement stationnaires.\\
Nous utiliserons la notation suivante pour la définition de réversibilité : $\pi(dx) = f(x)dx$. Et donc, pour tout mesurable $A$, $\pi(A) = \int_{x \in A} \pi(dx) = \int_{x \in A} f(x)dx$.\\
Nous utiliserons l'équation de balance détaillée pour en déduire la réversibilité: 
\begin{definition}
Équation de balance détaillée
\end{definition}

Une distribution $\pi$ est réversible selon une chaîne de Markov $\left\{ X_{t} \right\}$ en particulier si : $\pi(dx)\mathbb{P}(X_{t+1} \in dy | X_{t} = x) = \pi(dy)\mathbb{P}(X_{t+1} \in dx | X_{t} = y).$
\begin{lemme}
Si $\pi$ est réversible, alors $\pi$ est stationnaire.
\end{lemme}

\begin{proof}
Soit $\Omega$ l'espace d'état de la chaîne de Markov $\left\{ X_{t} \right\}$ considérée et $\pi$ réversible pour cette chaîne.
Pour $X_{t} \sim \pi$, et $C$ un ensemble mesurable, alors on a :

\begin{align*}
\mathbb{P}(X_{t+1} \in C) &= \mathbb{E}[\mathbf{1}(X_{t+1} \in C)] \\ 
&= \mathbb{E}[\mathbb{E}[\mathbf{1}(X_{t+1} \in C)|X_{t}]]
\\
&= \int_{x \in \Omega} \mathbb{E}[\mathbf{1}(X_{t+1} \in C)|X_{t} = x]\pi(dx)
\\
&= \int_{x \in \Omega} \mathbb{P}(X_{t+1} \in C | X_{t} = x)\pi(dx)
\\
&= \int_{x \in \Omega} \int_{y \in C} \mathbb{P}(X_{t+1} \in dy|X_{t} = x)\pi(dx)
\\
&= \int_{y \in C} \int_{x \in \Omega} \mathbb{P}(X_{t+1} \in dx|X_{t} = y)\pi(dy)
\\
&=  \int_{y \in C} \mathbb{P}(X_{t+1} \in \Omega|X_{t} = y)\pi(dy)
\\
&= \int_{y \in C} \pi(dy)
\\
&= \pi(C)
\end{align*}

D'où la stationnarité de $\pi$.\\

\end{proof}

Plusieurs types de chaînes réversibles existent telles que l'échantillonnage de Gibbs (Gibbs sampler), Metropolis-Hastings,etc.
\\
Ces chaînes sont présentées dans la suite.

\subsubsection{Échantillonnage de Gibbs}

On présente ici un échantillonneur de Gibbs, qui agit sur un espace d'états de la forme $C^{V}$. On appelle $\nu \in V$ une dimension du problème. Pour $X_{t} = x$, l'échantillonneur choisit une dimension $\nu$ uniformément sur $V$. Soit $L(x,\nu)$ l'ensemble des états qui sont exactement les n\oe{}uds de la configuration $x$ sauf en $\nu$, écrit de la manière suivante : $L(x,v) = \left\{ y : (\forall w \in V \setminus \left\{v\right\})(y(w) = x(w))\right\}.$ Pour $X_{t} = x$, l'état suivant $X_{t+1}$ est choisi selon $\pi$ à conditionné à être dans $L(x,v)$.
\\
On présente comme exemple le modèle d'Ising précédemment vu. Une dimension est alors un n\oe{}ud de la configuration.
On choisit donc un n\oe{}ud uniformément dans $V$, et on considère les états qui sont exactement $x$ en tous les autres n\oe{}uds autres que $\nu$. Pour le modèle d'Ising, la valeur en $\nu$ de x  est $1$ ou $-1$. On note ces configurations $x_{\nu \to 1}$ et $x_{\nu \to -1}$. On choisit alors l'état suivant entre $x_{\nu \to 1}$ et $x_{\nu \to -1}$, où le choix est fait proportionnellement à $\pi$. On a donc : 

$$\mathbb{P}(X_{t+1} = x_{\nu \to 1})  = \frac{\pi(\left\{x_{\nu \to 1}\right\})}{\pi(\left\{x_{\nu \to 1}\right\})+\pi(\left\{x_{\nu \to -1}\right\})}$$

Or, pour le modèle d'Ising,

$$\pi(\left\{x\right\})= \frac{1}{Z}\prod_{i \in V} exp(\mu X(i)) \prod_{\left\{i,j\right\} \in E} exp(\beta \mathbf{1}(x(i)=x(j)).$$

Après simplification, on obtient :
\begin{align*}
\mathbb{P}(X_{t+1} = x_{\nu \to 1}) &= \frac{exp(\mu)\prod_{\left\{\nu,j\right\} \in E} exp(\beta \mathbf{1}(x(j)=1))}{exp(\mu)\prod_{\left\{\nu,j\right\} \in E} exp(\beta \mathbf{1}(x(j)=1)) + exp(-\mu)\prod_{\left\{\nu,j\right\} \in E} exp(\beta \mathbf{1}(x(j)=-1))}
\\
&= \frac{exp(\beta n_{1} + \mu)}{exp(\beta n_{1} + \mu) + exp(\beta n_{-1} - \mu)}
\end{align*}
Où $n_{c}$ est le nombre de voisins de $\nu$ de label c.
Par exemple, si $\nu$ est adjacent à trois n\oe{}uds de label $1$ et un n\oe{}ud de label $-1$, alors la probabilité que $\nu$ se voit labelliser $1$ est $exp(3\beta + \mu)/(exp(3\beta + \mu)+exp(\beta - \mu))$.
Mettre ensuite en place cette méthode algorithmiquement est très simple.\\
Vérifions la réversibilité de la chaîne.
\begin{proof}
Les cas où la chaîne ne change pas d'état après un pas,(ie $X_{t} = x_{\nu \to 1}$  et $X_{t+1} = x_{\nu \to 1}$ ou $X_{t} = x_{\nu \to -1}$ et $X_{t+1} = x_{\nu \to -1}$) nous donnent immédiatement l'équation de balance détaillée, il ne reste alors qu'à traiter deux cas.
\begin{itemize}
\item Premier cas : $X_{t} = x_{\nu \to 1}$ et $X_{t+1} = x_{\nu \to -1}$.
\\

On a alors : $$ \pi(\left\{x_{\nu \to 1 } \right\}) \mathbb{P}(X_{t+1} = x_{\nu \to -1} | X_{t} = x_{\nu \to 1}) =\pi(\left\{x_{\nu \to 1 } \right\}) \frac{exp(-\mu + \beta n_{-1})}{exp(-\mu + \beta n_{-1})+exp(\mu + \beta n_{1})} $$
D'autre part : 
$$ \pi(\left\{x_{\nu \to -1 } \right\}) \mathbb{P}(X_{t+1} = x_{\nu \to 1} | X_{t} = x_{\nu \to -1}) =\pi(\left\{x_{\nu \to -1 } \right\}) \frac{exp(\mu + \beta n_{1} )}{exp( - \mu + \beta n_{-1})+exp(\mu + \beta n_{1} )} $$

On aura donc l'égalité entre ces termes si et seulement si on a :

$$ \pi(\left\{x_{\nu \to 1 } \right\})exp(-\mu + \beta n_{-1}) = \pi(\left\{x_{\nu \to -1 } \right\})exp(\mu + \beta n_{1} ) $$

Les simplifications suivantes s'opèrent dans les termes en $\pi$ : 

\begin{itemize}
\item simplification des $\frac{1}{Z}$
\item simplification des $\prod_{i \in V} exp(\mu X(i))$ sauf au n\oe{}ud $\nu$
\item simplification des $\prod_{\left\{i,j\right\} \in E} exp(\beta \mathbf{1}(x(i)=x(j))$ sauf aux arêtes ayant $\nu$ comme extrémité
\end{itemize}
~\\
On obtient alors :

$$exp(\mu + \beta n_{1} ) exp(-\mu + \beta n_{-1}) = exp(-\mu + \beta n_{-1})exp(\mu + \beta n_{1} ) $$

On a donc bien vérifié l'égalité et donc l'équation de balance détaillée.



\end{itemize}

Le deuxième cas : $X_{t} = x_{\nu \to -1}$ et $X_{t+1} = x_{\nu \to 1}$ se vérifie de la même manière.

~\\
Ayant vérifié tous les cas possibles, on obtient donc l'équation de balance détaillée qui conduit bien à la réversibilité de la chaîne.

\end{proof}

\newpage
\paragraph{\underline{\textbf{Programmation sous R}}}
~\\


\begin{lstlisting}
## X = graphe (rectangulaire ou carré), u = lancer uniforme sur [0,1], v = noeud de X ##
IsingGibbsUpdate <- function(X,u,v,beta,mu){ ## Permet de faire un pas de la chaîne de Markov associée selon le Gibbs Sampler##
  
  ##On entoure la matrice de valeurs aberrantes pour la manipuler plus facilement##
  ##On change les indices du point considéré pour se conformer à la nouvelle matrice##
  
  i = v[1] + 1
  j = v[2] + 1
  
  X = rbind(rep(0,dim(X)[2]),X)
  X = rbind(X,rep(0,dim(X)[2]))
  X = cbind(rep(0,dim(X)[1]),X)  
  X = cbind(X,rep(0,dim(X)[1]))

##Les indicatrices permettent de vérifier quelles valeurs ont les voisins du point considéré ##

  if(u< exp(mu + beta*((X[i+1,j]==1)*1 + (X[i-1,j]==1)*1 +(X[i,j+1]==1)*1 + (X[i,j-1]==1)*1))/((exp(mu + beta*((X[i+1,j]==1)*1 + (X[i-1,j]==1)*1 +(X[i,j+1]==1)*1 + (X[i,j-1]==1)*1))) + exp(-mu + beta*((X[i+1,j]==-1)*1 + (X[i-1,j]==-1)*1 +(X[i,j+1]==-1)*1 + (X[i,j-1]==-1)*1)))){
    X[i,j] =  1
  }
  else{
    X[i,j] = -1
  }
  
  ##On supprime les valeurs insérées##
  X=X[,-1]
  X=X[,-dim(X)[2]]
  X=X[-1,]
  X=X[-dim(X)[1],]
  
  X
}

\end{lstlisting}

\newpage

\begin{lstlisting}
IsingGibbssteps <- function(X,beta,mu,n){ ##effectue n pas de la chaîne de Markov pour le sampler de Gibbs, pour un graphe rectangulaire X##  

  for (i in 1:n){
  
  ##choix uniforme du noeud considéré##
    k = sample(1:dim(X)[1],1)
    l = sample(1:dim(X)[2],1)
  ##lancer uniforme sur [0,1]##
    u = runif(1)
  ##mise à jour du point##
    X=IsingGibbsUpdate(X,u,c(k,l),beta,mu)
    
  }

  X
}
\end{lstlisting}

\newpage

\begin{lstlisting}
IsingGibbsconditionnalstop <- function(X,beta,epsilon){ ##effectue un nombre aléatoire de pas de la chaîne de Markov pour le sampler de Gibbs, pour un graphe carré X##  
  
  ##On créé le vecteur Y qui contiendra les ratios de 1 pour chaque graphe##  
  Y=c(mean(X==1))
  bol = TRUE
  i=1
  
  while (bol){
	
	##Choix uniforme du noeud considéré##
    k = sample(1:dim(X)[1],1)
    l = sample(1:dim(X)[2],1)
	
	##lancer uniforme sur [0,1]##    
    u = runif(1)
    
    ##Mise à jour du noeud considéré##
    X=IsingGibbsUpdate(X,u,c(k,l),beta)
    
    ##Mise à jour du vecteur des ratios##
    Y = c(Y,mean(X==1))
    
    ##Si la ligne est décommentée, on obtient une représentation de la configuration X tout les (dim(X)[1]**2)/epsilon pas##
    ##if ((i%%((dim(X)[1]**2)/epsilon)==0)){image(X)}

	##Si on a effectué un nombre de pas suffisant, et que le nombre de 1 n'a pas varié énormément durant un certain nombre d'itérations, on s'arrête##
    if ((i>=((log(dim(X)[1]**2)*dim(X)[1]**2)/epsilon)) && ((mean(Y[i+1-floor((dim(X)[1]**(1/2))/epsilon):i])-Y[i])<epsilon)){
      break()
    }
    i=i+1
  }
  
  return(list(X,i))
  
}
\end{lstlisting}
\newpage





\subsubsection{Metropolis-Hastings}

Pour la méthode de Metropolis-Hastings, pour chaque configuration $x$ de la chaîne, on a une densité $q_{x}$. On propose ensuite un état suivant y, selon la densité $q_{x}$. L'état suivant sera donc $x$ ou $y$. La probabilité avec laquelle est choisie la configuration $y$ est donnée de telle sorte que la réversibilité s'applique. On la donne ci-dessous. Notez que, pour que cela fonctionne, il est nécessaire que si $q_{x}(y) >0$, alors $q_{y}(x) >0$ aussi. On a alors le changement vers $y$ avec probabilité : 
$$ min\left\{ 1, \frac{f_{\pi}(y)q_{y}(x)}{f_{\pi}(x)q_{x}(y)} \right\}$$

On notera aussi que $f_{\pi}$ peut-être normalisée ou non.
\\
En exemple, on considère à nouveau le modèle d'Ising. A chaque pas, on choisit un n\oe{}ud uniformément dans $V$ puis un label candidat pour ce n\oe{}ud est choisi uniformément dans $\left\{ -1,1 \right\}$. On a alors $q_{x}(y) = q_{y}(x) = 1/2$. On calcule ensuite, pour $n_{c}$ le nombre de voisins de $\nu$ de label $c$ (où c est le label proposé) et $n_{x(\nu)}$ le nombre de voisins de $\nu$ de même label que $\nu$ : 

$$\frac{f_{\pi}(y)}{f_{\pi}(x)} = \frac{exp(n_{c} \beta + c\mu)}{exp(n_{x(\nu)}\beta + x(\nu)\mu )}$$ 

L'algorithme est alors très simple à mettre en place.
D'abord, vérifions la réversibilité de la chaîne.
\begin{proof}
Les cas où la chaîne ne change pas d'état après un pas,(ie $X_{t} = x_{\nu \to 1}$  et $X_{t+1} = x_{\nu \to 1}$ ou $X_{t} = x_{\nu \to -1}$ et $X_{t+1} = x_{\nu \to -1}$) nous donnent immédiatement l'équation de balance détaillée, il ne reste alors qu'à traiter deux cas.
\begin{itemize}
\item Premier cas : $X_{t} = x_{\nu \to 1}$ et $X_{t+1} = x_{\nu \to -1}$.
\\

On a alors : $$ \pi(\left\{x_{\nu \to 1 } \right\}) \mathbb{P}(X_{t+1} = x_{\nu \to -1} | X_{t} = x_{\nu \to 1}) =\pi(\left\{x_{\nu \to 1 } \right\}) min\left\{ 1, \frac{exp(-\mu + \beta n_{-1})}{exp(\mu + \beta n_{1})} \right\} $$
D'autre part : 
$$ \pi(\left\{x_{\nu \to -1 } \right\}) \mathbb{P}(X_{t+1} = x_{\nu \to 1} | X_{t} = x_{\nu \to -1}) =\pi(\left\{x_{\nu \to -1 } \right\}) min\left\{ 1, \frac{exp(\mu + \beta n_{1})}{exp(-\mu + \beta n_{-1})} \right\} $$

On aura donc l'égalité entre ces termes si et seulement si on a :

$$ \pi(\left\{x_{\nu \to 1 } \right\}) min\left\{ 1, \frac{exp(-\mu + \beta n_{-1})}{exp(\mu + \beta n_{1})} \right\} = \pi(\left\{x_{\nu \to -1 } \right\}) min\left\{ 1, \frac{exp(\mu + \beta n_{1})}{exp(-\mu + \beta n_{-1})} \right\} $$

Les simplifications suivantes s'opèrent dans les termes en $\pi$ : 

\begin{itemize}
\item simplification des $\frac{1}{Z}$
\item simplification des $\prod_{i \in V} exp(\mu X(i))$ sauf au n\oe{}ud $\nu$
\item simplification des $\prod_{\left\{i,j\right\} \in E} exp(\beta \mathbf{1}(x(i)=x(j))$ sauf aux arêtes ayant $\nu$ comme extrémité
\end{itemize}
~\\
On obtient alors :

$$exp(\mu + \beta n_{1}) min\left\{ 1, \frac{exp(-\mu + \beta n_{-1})}{exp(\mu + \beta n_{1})} \right\} = exp(-\mu + \beta n_{-1}) min\left\{ 1, \frac{exp(\mu + \beta n_{1})}{exp(-\mu + \beta n_{-1})} \right\} $$

Par propriété du min, on obtient alors l'égalité entre les termes, qui conduit à l'équation de balance détaillée. 

\end{itemize}

~\\
Le deuxième cas : $X_{t} = x_{\nu \to -1}$ et $X_{t+1} = x_{\nu \to 1}$ se vérifie de la même manière.

~\\
Ayant vérifié tous les cas possibles, on obtient donc l'équation de balance détaillée qui conduit bien à la réversibilité de la chaîne.

\end{proof}


\newpage
\paragraph{\underline{\textbf{Programmation sous R}}}
~\\


\begin{lstlisting}
## X = graphe (rectangulaire ou carré), u = lancer uniforme sur [0,1], v = noeud de X ##

MetropolisHastingsUpdate <- function(X,u,v,beta){  ## Permet de faire un pas de la chaîne de Markov associée selon Metropolis-Hastings     
  
  ##On entoure la matrice de valeurs aberrantes pour la manipuler plus facilement##
  ##On change les indices du point considéré pour se conformer à la nouvelle matrice##
  
  i = v[1] + 1
  j = v[2] + 1
  
  X = rbind(rep(0,dim(X)[2]),X)
  X = rbind(X,rep(0,dim(X)[2]))
  X = cbind(rep(0,dim(X)[1]),X)  
  X = cbind(X,rep(0,dim(X)[1]))
  
  ##On choisit une couleur candidate pour le noeud v##
  
  c=sample(c(-1,1),1,prob=c(1/2,1/2))
  
  ##Les indicatrices permettent de vérifier quelles valeurs ont les voisins du point considéré ##
  
  if(u< exp(beta*((X[i+1,j]==c)*1 + (X[i-1,j]==c)*1 +(X[i,j+1]==c)*1 + (X[i,j-1]==c)*1))/exp(beta*((X[i+1,j]==X[i,j])*1 + (X[i-1,j]==X[i,j])*1 +(X[i,j+1]==X[i,j])*1 + (X[i,j-1]==X[i,j])*1))){
    X[i,j] =  c
  }
  
  ##On supprime les valeurs insérées##
  X=X[,-1]
  X=X[,-dim(X)[2]]
  X=X[-1,]
  X=X[-dim(X)[1],]
  
  X
} 
\end{lstlisting}

\newpage
\begin{lstlisting}
MetropolisHastingssteps <- function(X,beta,n){ ##effectue n pas de la chaîne de Markov pour Metropolis-Hastings, pour un graphe rectangulaire X##  
  
  for (i in 1:n){
  
  ##choix uniforme du noeud##
    k = sample(1:dim(X)[1],1)
    l = sample(1:dim(X)[2],1)
    
  ##lancer uniforme sur [0,1]##
    u = runif(1)
    
  ##mise à jour de la configuration X##
    X=MetropolisHastingsUpdate(X,u,c(k,l),beta)
  }
  
  X
  
}

\end{lstlisting}

\newpage
\begin{lstlisting}

MetropolisHastingsconditionnalstop <- function(X,beta,epsilon){ ##effectue un nombre aléatoire de pas de la chaîne de Markov pour Metropolis-Hastings, pour un graphe carré X## 

 ##On créé le vecteur Y qui contiendra les ratios de 1 pour chaque graphe##  
  Y=c(mean(X==1))
  bol = TRUE
  i=1
  
  
  while (bol){
  
  ##Choix uniforme du noeud considéré##
    k = sample(1:dim(X)[1],1)
    l = sample(1:dim(X)[2],1)
    
  ##lancer uniforme sur [0,1]##  
    u = runif(1)
   
  ##Mise à jour du noeud considéré##
    X=MetropolisHastingsUpdate(X,u,c(k,l),beta)
    
  ##Mise à jour du vecteur des ratios##
    Y = c(Y,mean(X==1))
    
  ##Si la ligne est décommentée, on obtient une représentation de la configuration X tout les (dim(X)[1]**2)/epsilon pas##
  ##if ((i%%((dim(X)[1]**2)/epsilon)==0)){image(X)}##
  
  ##Si on a effectué un nombre de pas suffisant, et que le nombre de 1 n'a pas varié énormément durant un certain nombre d'itérations, on s'arrête##
    if ((i>=((log(dim(X)[1]**2)*dim(X)[1]**2)/epsilon)) && ((mean(Y[i+1-floor((dim(X)[1]**(1/2)))/epsilon:i])-Y[i])<epsilon)){
      break()
    }
    i=i+1
  }
  
  return(list(X,i))
  
}


\end{lstlisting}

\newpage
\paragraph{\underline{\textbf{Discussion du critère d'arrêt dans "IsingGibbsconditionnalstop"}}}
\paragraph{\underline{\textbf{et "MetropolisHastingsconditionnalstop"}}}
~\\
\\
Le premier critère pour l'arrêt du programme est si le nombre de pas $i$ vérifie : $i>=((log(n^{2})*n^{2})/\epsilon)$, où $n^{2}$ est la taille du graphe carré considéré et $\epsilon$ est la "précision" que l'utilisateur souhaite. \\
Le terme en $n^{2}*log(n^{2})$ provient du problème du collectionneur. En effet, on souhaite, pour atteindre un certain équilibre, avoir donné la chance à chacun des n\oe{}uds d'avoir subi un changement. Cela revient à avoir sélectionné chacun des n\oe{}uds au moins une fois, ce qui revient au problème du collectionneur, où en moyenne le temps d'obtention de tous les éléments de la collection se fait en $n*log(n)$ si $n$ est la taille de la collection complète.\\
Le terme en $\epsilon$ permet de s'assurer d'avoir obtenu la visite complète de tous les n\oe{}uds, mais servira plus amplement dans l'autre condition d'arrêt présentée ci-après.\\
Le second critère est si le vecteur des ratios de $1$ vérifie :
$$|Y[i] - \frac{1}{\lfloor \frac{n}{\epsilon}\rfloor}\sum_{k=i+1-\lfloor\frac{n}{\epsilon}\rfloor}^{i}Y[k]|<\epsilon$$
C'est-à-dire, si le ratio de $1$ n'a pas évolué de plus de $\epsilon$ en moyenne depuis le temps $i+1-\lfloor\frac{n}{\epsilon}\rfloor$.
On parle donc de "précision" $\epsilon$ puisque plus cette valeur est petite, plus le temps requis pour que peu de changements s'effectuent en moyenne est grand.\\
Le terme en $n$ (où $n^{2}$ est la taille de la matrice) est ajouté pour obtenir une dépendance à la dimension dans la précision.



\subsubsection{Variables aléatoires auxiliaires / Auxiliary random variables}

Dans la plupart des applications du MCMC, la densité ciblée $X$ a une structure multiplicative. Pour ces densités, il est possible d'ajouter un vecteur $Y$ de variables aléatoires supplémentaires telles que $(X,Y)$ a une distribution jointe plus simple.
Par exemple, on considère à nouveau le modèle d'Ising avec $\mu = 0$. Pour chaque arête $\left\{ i,j \right\}$, on crée une variable aléatoire auxiliaire $Y(\left\{i,j\right\})$ telle que sa distribution conditionnée sur $X$ soit uniforme sur $[0,1]$ si $X(i) \neq X(j)$ et uniforme sur $[0,exp(\beta)]$ si on a l'égalité.\\
La densité jointe est alors uniforme sur : 
$$X \in \left\{0,1\right\}^{V} \text{et } Y \in \left\{ [0, \infty) : (\forall\left\{i,j\right\})(y\left\{i,j\right\}) \leq min(exp(\beta),1))\right\}$$
La chaîne de Markov est alors la suivante : pour $X$ donné, il suffit de choisir une nouvelle valeur de $Y$ sachant $X$, puis choisir un nouveau $X$ conditionné sur $Y$.\\
De par la construction de la chaîne, tirer $Y$ sachant $X$ est direct. Cependant, tirer $X$ sachant $Y$ est une autre histoire : prenons pour exemple le modèle d'Ising avec $\beta >0$. On a alors $exp(\beta) >1$. Lorsque $y(\left\{i,j\right\}) \in [0,1]$, alors il est possible que $x(i)=x(j)$ ou que $x(i) \neq x(j)$, mais lorsque $y(\left\{i,j\right\}) > 1$, alors on a forcément que $x(i)=x(j)$. Les arêtes avec  $y(\left\{i,j\right\}) > 1$ séparent le graphe en groupes de composants connectés tels que chacun des composants doivent avoir le même label.\\
Il suffit donc de séparer le graphe en groupes de composants connectés en se servant des arêtes $y(\left\{i,j\right\}) > 1$, puis il faut choisir un label uniformément sur $\left\{-1,1\right\}$ pour les composantes de ces groupes.\\

\subsubsection{Shift Chains}

Un autre type de chaîne utile pour les modèles répulsifs est le shift. Pour illustrer le modèle, on considère le modèle hard-core où chaque n\oe{}ud du graphe est soit occupé ($x(\nu) = 1$) soit inoccupé ($x(\nu) = 0$). Chaque n\oe{}ud occupé contribue d'un facteur $\lambda$ à la densité. Construire un pas à l'aide de l'échantillonneur de Gibbs est alors très simple.

\floatname{algorithm}{HCGM-Gibbs}
\begin{algorithm}
\caption{Entrée : état $x$ Sortie :nouvel état $x$}
\begin{algorithmic}[1]
\STATE Tirer $\nu \leftarrow Unif(V)$
\STATE Tirer $U \leftarrow Unif([0,1])$
\STATE $N_{1} \leftarrow$ nombre de voisins de $\nu$ de label $1$ dans $x$
\IF{$U < \lambda/(\lambda + 1)$ et $n_{1} = 0$}
\STATE $x_{\nu} \leftarrow 1$
\ENDIF
\end{algorithmic}
\end{algorithm}


Dès lors qu'un des voisins de $\nu$ est occupé, la chaîne ne peut changer la valeur de $\nu$.\\
Le shift autorise un échange de label entre $\nu$ et son voisin occupé avec une certaine probabilité $p_{shift}$. Lorsque deux voisins de $\nu$ ou plus sont occupés, aucun changement n'est possible et $\nu$ reste inoccupé. On obtient alors l'algorithme suivant.

\newpage
\floatname{algorithm}{HCGM-Shift}
\begin{algorithm}
\caption{Entrée : état $x$ Sortie :nouvel état $x$}
\begin{algorithmic}[1]
\STATE Tirer $\nu \leftarrow Unif(V)$
\STATE Tirer $U \leftarrow Unif([0,1])$
\STATE $S \leftarrow Bern(p_{shift})$
\STATE $N_{1} \leftarrow$ nombre de voisins de $\nu$ de label $1$ dans $x$
\IF{$U < \lambda/(\lambda + 1)$ et $n_{1} = 0$}
\STATE $x_{\nu} \leftarrow 1$
\ELSIF{$U < \lambda/(\lambda + 1)$ et $n_{1} = 1$ et $S = 1$}
\STATE $w \leftarrow$ l'unique voisin de $\nu$ de label $1$
\STATE $x(\nu) \leftarrow 1$
\STATE $x(w) \leftarrow -1$
\ENDIF
\end{algorithmic}
\end{algorithm}




\newpage
\section{Chapitre 2 : Procédures de rejet}

\subsection{Théorème et exemple simple}

\begin{theorem}
Soit $\nu$ une mesure finie sur un ensemble $B$ et soit $A$ un sous-ensemble de $B$ tel que $\nu(A) > 0$. Soit $X_{1},X_{2},\ldots$ des variables aléatoires iid tirées selon $\nu$ sur l'ensemble $B$. Soit enfin $T = inf \left\{t :  X_{t} \in A \right\}.$ Alors : 

$$ X_{T} \sim [ X_{1} | X_{1} \in A ]$$ 
\end{theorem}


C'est à dire, si on tire successivement des valeurs dans $B$ selon $\nu$ et que l'on rend la première valeur dans $A$, alors le résultat provient de la mesure $\nu$ conditionnée à être dans $A$.

\begin{proof}
Cela suit du théorème fondamental de la simulation parfaite en ayant $U_{1},U_{2},\dots$ des tirages dans $B$ selon $\nu$, $b(U) = \mathbf{1}(U \in A)$,$g(U) = U$ et $f(X,U) = X$.
\end{proof}

Le nombre moyen de pas nécessaire pour obtenir un tirage dans $A$ dépend de la taille de $A$ par rapport à la taille de $B$.

\begin{theorem}
La probabilité qu'un tirage $X$ dans $B$ selon $\nu$ soit dans $A$ est $\nu(A)/\nu(B)$. Le nombre moyen de tirages nécessaires est $\nu(B)/\nu(A)$.
\end{theorem}


\begin{proof}
La première moitié du théorème découle simplement de la définition de mesure de probabilité. La seconde provient du fait que $T$ soit une loi géométrique de paramètre $\nu(A)/\nu(B)$, et donc sa moyenne est $\nu(B)/\nu(A)$.
\end{proof} 

\paragraph{Exemple : tirage uniforme sur le cercle unité}
~\\
~\\
On choisit $B = [-1,1] \times [-1,1]$ et $A = \left\{ (x,y) \in \mathbb{R}^{2} | x^{2} + y^{2} \leq 1 \right\}$.\\
L'aire de $B$ est 4 tandis que l'aire du cercle unité est $\pi$. Le nombre moyen de lancers est alors $2 * (4/\pi) = 2*1.273$, où le facteur $2$ provient du nombre d'uniformes nécessaires pour obtenir un tirage dans $B$.\\
On propose une mise en place sous R en fin de chapitre.\\
On déduit de cette méthode un moyen de générer une réalisation de Cauchy standard : 

\begin{theorem}
Si $(X,Y)$ est un tirage aléatoire sur le cercle unité de $\mathbb{R}^{2}$, alors $X/Y$ est un tirage aléatoire selon la loi de Cauchy standard.
\end{theorem}


\begin{proof}
Soit $\phi$ une fonction borélienne bornée sur $\mathbb{R}$. On a, pour $(X,Y)$ variable aléatoire uniforme sur le disque unité : 

$$ \mathbb{E}\left[\phi\left(\frac{Y}{X} \right)\right] = \frac{1}{\pi} \int_{\mathbb{R}^{2}} \phi\left( \frac{y}{x}\right)\mathbf{1}_{\left\{ x^{2} + y^{2} \leq 1 \right\}}dxdy$$

On passe en coordonnées polaires : $y = r\sin(\theta)$, $x = r\cos(\theta)$. On a alors $dxdy = rdrd\theta$ et on passe d'une intégrale sur $\mathbb{R}^{2}$ vers une intégrale sur $\mathbb{R}^{+} \times [0 , 2\pi]$, d'où : 

\begin{align*}
\mathbb{E}\left[\phi\left(\frac{Y}{X} \right)\right] &= \frac{1}{\pi} \int_{\mathbb{R}^{+} \times [0 , 2\pi]} \phi(\tan(\theta))\mathbf{1}_{\left\{r \leq 1\right\}} rdrd\theta \\
&= \frac{1}{2\pi} \int_{0}^{2\pi} \phi(\tan(\theta))d\theta \\
&= 2 * \frac{1}{2\pi} \int_{-\pi/2}^{\pi/2} \phi(\tan(\theta))d\theta
\end{align*}
On pose ensuite le changement de variables $u = \tan(\theta)$ pour enfin obtenir : 

$$ \mathbb{E}\left[\phi\left(\frac{Y}{X} \right)\right] = \int_{\mathbb{R}^{+}}\phi(u)\frac{1}{\pi (1+u^{2})}du$$

Ce qui donne bien la densité d'une loi de Cauchy standard.
On obtient aussi le résultat pour $X/Y$ puisque l'inverse de la loi de Cauchy standard reste la loi de Cauchy standard.

\end{proof}


\subsection{Méthode de rejet pour variables aléatoires à densité}

La méthode de rejet de base n'inclut pas les variables aléatoires à densité. Cependant, celles-ci sont traitées aisément à l'aide des variables aléatoires auxiliaires. Pour cela, on considère le théorème suivant, provenant de la théorie de la mesure.

\begin{theorem}[Théorème fondamental de la simulation de Monte Carlo]
Soit $X$ variable aléatoire de densité (possiblement non-normalisée) $f_{X}$ par rapport à une mesure $\nu$ sur $\Omega$. Si on a $[Y|X] \sim Unif([0,f_{X}])$, alors $(X,Y)$ est un lancer provenant de la mesure produit $\nu ~ \times$ Unif sur l'ensemble \\ $\left\{(x,y) : x\in \Omega, 0 \leq y \leq f_{x} \right\}$.
\end{theorem}


Essentiellement, ce théorème nous dit que pour obtenir un lancer selon une certaine densité, il suffit de tirer selon une loi uniforme sur un espace plus grand. De plus, la densité de $X$ n'est pas forcément normalisée, ce qui sera essentiel dans les applications qui suivent. 
\\
~\\
Pour comprendre comment cela fonctionne, nous allons traiter un exemple en toute généralité, puis un exemple concret. \\
Soit $\mu$ une mesure sur un ensemble $\Omega$. On suppose que, pour une densité $g$, il est possible de générer un lancer provenant de la densité produit $\mu ~\times$ Unif sur $\Omega_{g} = \left\{ (x,y) : x \in \Omega, 0 \leq y \leq g \right\}$.\\
Une fois que ce lancer est obtenu, $X$ provient alors de la densité $g$. Mais supposons ici que le but est d'obtenir un lancer selon une densité $f$.\\
Notons d'abord que, si $(X,Y)$ est de mesure $\mu ~ \times$ Unif sur $\Omega_{g}$, alors $(X,2Y)$ est de mesure $\mu ~ \times$ Unif sur $\Omega_{2g} = \left\{ (x,y) : x \in \Omega, 0 \leq y \leq 2g \right\}$. On peut même généraliser : pour toute constante $c > 0$, $(X,cY)$ est de mesure $\mu ~ \times$ Unif sur $\Omega_{cg}$.\\
Supposons alors que $c \geq sup_{x \in \Omega} f(x)/g(x)$. Alors : 

$$ \Omega_{f} \subseteq \Omega_{cg}$$

On peut alors utiliser une méthode de rejet pour obtenir une réalisation provenant de la densité $f$. Il suffit de générer $X$ selon la densité $g$, puis de tirer uniformément $Y$ sur $[0,c*g(X)]$. Si $Y$ est aussi dans $[0,f(X)]$ (de sorte que $(X,Y) \in \Omega_{f})$ alors on accepte $(X,y)$, ce qui signifie que $X$ est un lancer selon la densité $f$. \\
Pour tester si le lancer uniforme de $[0,c*g(X)]$ est dans $[0,f(X)]$, il n'est pas nécessaire de vérifier si l'uniforme tombe effectivement dedans : on peut simplement tirer une variable aléatoire $C$ suivant une loi de Bernouilli de paramètre $f(X)/[c*g(X)]$. Si $C = 1$, on considère que $X$ est un lancer provenant de la densité $f$.\\
La probabilité d'acceptation varie inversement avec $c$, le meilleur choix possible pour $c$ est alors $sup_{x \in \Omega} f(x)/g(x)$. La valeur de $c$ n'est pas toujours évidente à calculer. Dans ce cas, une majoration suffit : l'algorithme fera simplement plus de lancers en moyenne.\\
Nous allons maintenant calculer la probabilité d'acceptation d'un lancer.

\begin{lemme}
Soit $Z_{f} = \int_{x \in \Omega} f(x) \nu(dx)$ et $Z_{g} = \int_{x \in \Omega} g(x) \nu(dx)$, les constantes de normalisation pour les densités (possiblement) non-normalisées $f$ et $g$.
Alors, pour la méthode par rejet présentée précédemment, la probabilité d'acceptation est $Z_{f}/[c*Z_{g}]$ et le nombre moyen de lancers est $c*Z_{g}/Z_{f}$.
\end{lemme}



\begin{proof}
Pour $C$, la loi de Bernouilli présentée précédemment, le problème est de calculer $\mathbb{P}(C=1)$. 

\begin{align*}
\mathbb{P}(C=1) &= \mathbb{E}[\mathbb{E}[\mathbf{1}(C=1)|X]] \\
&= \int_{x \in \Omega} \mathbb{P}(C=1|X=x)\mathbb{P}(X \in dx) \\
&= \int_{x \in \Omega} f(x)/[cg(x)](g(x)/Z_{g})\nu(dx) \\
&= [cZ_{g}]^{-1}\int_{x \in \Omega} f(x)\nu(dx) \\
&= Z_{f}/[c*Z_{g}]
\end{align*}

\end{proof}

\subsubsection{Exemple : D'une Cauchy vers une Normale}

Un exemple simple est la situation suivante : on suppose qu'un utilisateur peut générer une réalisation d'une loi de Cauchy standard de densité $g(x) = [\pi(1+x^{2})]^{-1}$. Cet utilisateur souhaite générer une réalisation d'une loi Normale standard de densité $f(x) = \frac{1}{\sqrt{2\pi}} \exp(-x^{2}/2)$.
D'abord, la normalisation n'est pas nécessaire, donc soit $g_{1}(x) = (1+x^{2})^{-1}$ et $f_{1}(x) = \exp(-x^{2}/2)$. On a que $f_{1}(x)/g_{1}(x) \leq 2 , \forall x$. On implémente alors sous R l'algorithme : 


\newpage

\paragraph{\underline{\textbf{Mise en place de la méthode de rejet sous R : d'une Cauchy vers une Normale}}}
~\\
\begin{lstlisting}
normalfromcauchy <- function(){## Fonction permettant de générer une réalisation de la loi normale standard à partir de la loi de Cauchy standard par méthode de rejet ##

##penser à changer le return pour obtenir une réalisation (return(x)) ou le temps mis à l'obtenir (return(n))##
  
  ##On initialise les valeurs ##
  C=0
  x=0
  n=0
  
  ##Tant que la bernouilli n'est pas 1##
  while(C!=1){
    
    ##On génère une Cauchy standard##
    x = rcauchy(1)
    
    ##On génère une Bernouilli du paramètre indiqué##
    C = rbinom(1,1,((sqrt(exp(1))/2)*(1+x^2)*exp(-(1/2)*x^2)))
    
    ##mise à jour du nombre de lancers##
    n=n+1
  }
  return(x)
}



##On calcule le nombre moyen de lancers nécessaires pour atteindre la sortie de l'algorithme précédent##

##On initialise le nombre moyen##
s=0

##On lance 10^6 fois l'algorithme qui retourne le nombre de lancers effectué ##
for (i in 1:10^6) {
  s=s+normalfromcauchy()
}

##On moyenne sur les 10^6 lancers
s = s/10^6



##On créé un histogramme illustrant 10^5 lancers de l'algorithme rendant une réalisation de la normale standard##

##On initialise le tableau qui contiendra les 10^5 lancers
t = matrix(0,1,10^5)

##On remplit le tableau en lançant 10^5 fois l'algorithme##
for (i in 1:10^5) {   #changer le return pour obtenir des réalisations
  t[i]=normalfromcauchy()

}

##On produit l'histogramme du tableau t et on rajoute la courbe de la densité de la loi normale standard##
hist(t,prob=T)

curve(dnorm(x, 0, 1), col="red",xlim=c(-4,4),add=T)


\end{lstlisting}

\newpage

L'histogramme rendu après un lancer du programme est le suivant : 

\includegraphics[scale=1]{graphiques/normaltocauchy.jpeg}

On obtient bien l'allure de la densité d'une loi normale standard.

On récupère aussi le nombre moyen de lancers requis afin d'obtenir une réalisation : 2.50588. On peut retrouver ce résultat de manière théorique en calculant la probabilité d'acceptation : 

\begin{align*}
\mathbb{P}(C=1) &= \int_{x \in \mathbb{R}} \mathbb{P}(X \in dx, C = 1) \\
&= \int_{x \in \mathbb{R}} g(x) \frac{1}{2}(1+x^{2})\exp(-x^{2}/2)dx \\
&= (2\pi)^{-1/2}
\end{align*}

Le nombre moyen de lancers est donc $\sqrt{2\pi} = 2.506\ldots$\\
Il est possible d'accélérer l'algorithme : on a en fait $sup_{x} (1+x^{2})\exp(-x^{2}/2) = 2/\sqrt{e}$. En remplaçant dans l'algorithme (mais aussi en calculant exactement l'intégrale), le nombre moyen de lancers devient $\sqrt{2\pi/e} = 1.520 \ldots$

\subsection{Union d'ensembles}

Une autre manière d'utiliser les méthodes de rejet est de prendre en compte les multiples manières d'obtenir une réalisation. Par exemple, on considère le problème de génération selon une mesure $\nu$ sur $S_{1} \cup \ldots \cup S_{n}$, où les ensembles $S_{i}$ sont de mesure finie $\nu(S_{i})$.\\
On considère l'algorithme suivant : on tire $I$ dans $\left\{1,2,\ldots,n\right\}$ de telle sorte que $\mathbb{P}(I=i) \propto \nu(S_{i})$. Puis, conditionnellement sur $I$, on tire $X$ dans $S_{I}$ selon la mesure $\nu$.\\
Le problème est que $X$ n'est pas tiré selon $\nu$ sur $S_{1} \cup \ldots \cup S_{n}$, car il y a plusieurs manières pour lesquelles $X$ aurait pu être choisi. Par exemple, si $X \in S_{1}$ et $X \in S_{2}$ mais $X \notin S_{3} \ldots S_{n}$, alors $I$ aurait pu être $1$ ou $2$ lors du choix de $X$.\\

\begin{lemme}
Pour la procédure précédente, $X$ est un tirage selon $\nu$ sur $S_{1} \cup \ldots \cup S_{n}$ avec densité $f(x) = \#\left\{i : x \in S_{i} \right\}/\nu(S_{1} \cup \ldots \cup S_{n})$
\end{lemme}


\begin{proof}
Soit $x \in S_{1} \cup \ldots \cup S_{n}$ et $C = \nu(S_{1} \cup \ldots \cup S_{n})$ Alors :

\begin{align*}
\mathbb{P}(X \in dx) &= \sum_{i=1}^{n} \mathbb{P}(X \in dx, I=i) \\
&= \sum_{i=1}^{n} \mathbb{P}(I=i)\mathbb{P}(X \in dx|I=i)\\
&= \sum_{i=1}^{n} C^{-1}\nu(S_{i}) \mathbf{1}(x \in S_{i}) \nu(dx)\nu(S_{i})^{-1}\\
&= C^{-1}\#\left\{i : x \in S_{i} \right\}
\end{align*}

\end{proof}

Par exemple, supposons que l'on veuille tirer uniformément sur les 3 cercles unités respectivement centrés en $(0,0)$,$(1/2,0)$ et $(1/2,1/2)$.\\
On tire alors $I \sim Unif(\{1,2,3\})$. Si $I=1$, on tire $X$ sur le cercle unité centré en $(0,0)$. Pour $I=2$ ou $I=3$, on tire $X$ sur le cercle centré en $(1/2,0)$ et $(1/2,1/2)$ respectivement. Après, avoir tiré $I$ puis $X$ conditionnellement à $I$, on accepte $X$ avec probabilité égale à l'inverse du nombre de cercles où se situe $X$.

\subsection{Simulation à l'aide des inégalités de Markov et Chernoff}

\subsubsection{L'inégalité de Markov en tant que procédure de rejet}

\begin{lemme}[Inégalité de Markov]
On considère une variable aléatoire $X$ non-négative avec probabilité $1$ et d'espérance finie $\mathbb{E}[X]$. Alors, pour tout $a >0$ :

$$ \mathbb{P}(X \geq a) \leq \frac{\mathbb{E}[X]}{a}$$
\end{lemme}



Du point de vue simulation, l'inégalité de Markov nous permet de tirer depuis $[X|X \geq a]$. C'est à dire, si $X$ à pour densité $f_{X}(x)$ alors le but est de tirer depuis la densité non-normalisée $f_{X}(x)\mathbf{1}(x>a)$. Pour pouvoir appliquer une méthode de rejet, on utilise la densité non-normalisée $xf_{X}(x)$. On a alors que $f_{X}(x)\mathbf{1}(x \geq a)/[xf_{X}(x)]$ vaut $0$ lorsque $x<a$, et $1/x$ lorsque $x \geq a$. Le produit n'est donc jamais supérieur à $1/a$ ce qui permet d'utiliser la méthode de rejet suivante : 

\floatname{algorithm}{Méthode rejet Inégalité de Markov}
\begin{algorithm}
\caption{Entrées : $f$,$a$ Sortie : $X \sim f$ sachant $X > a$ }
\begin{algorithmic} 
\WHILE{$C \neq 1$}

\STATE Tirer $X$ selon la densité non-normalisée $xf(x)$
\STATE Tirer $C \sim Bern(a\mathbf{1}(X \geq a)/x)$ 

\ENDWHILE
\end{algorithmic}
\end{algorithm}

On remarque que la constante de normalisation pour la densité $xf(x)$ est simplement : $\int xf(x)\nu(dx) = \mathbb{E}[X]$. On a alors :

\begin{align*}
\mathbb{P}(C=1) &= \int_{x \geq 0} \frac{xf_{X}(x)}{\mathbb{E}[X]} \frac{a\mathbf{1}( X > a)}{x} = \int_{x \geq a} \frac{af_{X}(x)}{\mathbb{E}[X]} = \frac{a \mathbb{P}(X > a)}{\mathbb{E}[X]}\\
\end{align*}

Puisque la chance d'accepter est au plus $1$, cette fraction est aussi au plus $1$, ce qui montre l'inégalité de Markov.

\subsubsection{Les inégalités de Chernoff en tant que procédure de rejet}
Les inégalités de Chernoff donnent des limites supérieures sur la probabilité qu'un somme de variables aléatoires est supérieure (ou inférieure) à une certaine valeur. Soit $S_{n} = X_{1} + \ldots + X_{n}$, où $X_{1},\ldots,X_{n}$ sont iid; alors le but est de générer un tirage selon $S_{n}$ tel que $S_{n} \geq a$ ou $S_{n} \leq a$.\\
L'inégalité de Markov peut aussi être appliquée à une somme de variables aléatoires, mais la borne donnée n'est pas aussi précise que celle obtenue avec les inégalités de Chernoff.

\begin{lemme}[Inégalités de Chernoff]
Soit une variable aléatoire $X$ telle que $\mathbb{E}[e^{tX}]$ soit finie pour $t \in [a,b]$, où $a$ est négatif et $b$ positif. Alors pour un certain $c \in \mathbb{R}$, et en notant par $mgf_{X}(t)$ la fonction génératrice des moments pour la variable aléatoire $X$, 

\begin{align*}
\mathbb{P}(X \geq c) \leq mgf_{X}(t)/\exp(tc), ~\forall t \in [0,b] \\
\mathbb{P}(X \leq c) \leq mgf_{X}(t)/\exp(tc), ~\forall t \in [a,0] 
\end{align*}
\end{lemme}


\begin{proof}
Tout d'abord : $\mathbb{P}(X \geq c) = \mathbb{P}(tX \geq tc)$ pour tout $t$ positif. De plus, $\mathbb{P}(tX \geq tc) = \mathbb{P}(\exp(tX) \geq \exp(tc))$. Enfin, on applique l'inégalité de Markov pour obtenir $\mathbb{P}(\exp(tX) \geq \exp(tc)) \leq \mathbb{E}[\exp(tX)]/\exp(tc)$. Le résultat pour l'autre inégalité est démontré de la même manière.
\end{proof}

Nous considérons à présent ce qu'il se passe lorsque l'on considère les inégalités de Chernoff pour une somme de variables aléatoires indépendante. On rappelle d'abord un résultat sur les fonctions génératrices des moments.\\

\begin{lemme}
Soit $S_{n} = X_{1} + \ldots + X_{n}$, où les $\{X_{i}\}$ sont iid de fonction génératrice des moments finie. Alors on a : $\mathbb{E}[\exp(tS_{n})] = [\mathbb{E}[\exp(tX_{i})]]^{n}$
\end{lemme}


On applique ce lemme aux inégalités de Chernoff pour obtenir : 

\begin{lemme}
Soit $S_{n} = X_{1} + \ldots + X_{n}$, où les $\{X_{i}\}$ sont iid de fonction génératrice des moments $mgf_{X_{i}}(t)$ finie pour $t \in [a,b]$, où $a$ est négatif et $b$ positif. Alors on a : 

\begin{align*}
\mathbb{P}(S_{n} \geq \alpha n) \leq  \left(\frac{mgf_{X_{i}}(t)}{\exp(t \alpha)}\right)^{n}, ~\forall t \in [0,b]\\
\mathbb{P}(S_{n} \leq \alpha n) \leq  \left(\frac{mgf_{X_{i}}(t)}{\exp(t \alpha)}\right)^{n}, ~\forall t \in [a,0]
\end{align*}
\end{lemme}


Supposons que $X_{i} \sim f_{X}$. Le but est d'utiliser la fonction génératrice des moments pour obtenir un meilleur algorithme de rejet. Pour cela, il doit être possible de générer une réalisation selon la densité $g_{t}(x) \propto e^{tx}f_{x}(x)$. Lorsque $t$ est grand, cette densité aura tendance à prendre de plus grandes valeurs. Lorsque $t$ est grand dans les négatifs, $g_{t}$ tendra à prendre des valeurs plus faibles.\\
Soit $t>0$. Pour $x \geq a$, alors $g_{t}(x) = e^{tx} f_{X}(x) \geq e^{ta}f_{X}(x)$. Alors une réalisation de $g_{t}$ peut être acceptée comme réalisation de $f_{X}$ avec probabilité $e^{ta}/e^{tx}$. Si la probabilité que $x$ soit bien plus grand que $a$ est faible, alors la probabilité d'acceptation sera très proche de $1$. Une méthode similaire s'applique lorsque $t<0$. On en déduit alors l'algorithme suivant :

\floatname{algorithm}{Méthode rejet Inégalités de Chernoff}
\begin{algorithm}[]
\caption{\newline Entrées : $f_{X}$,$a$,$t$ \newline  Sortie : $S_{n} = X_{1} + \ldots + X_{n} $sachant $S_{n} \geq a$ (lorsque $t >0$) ou sachant $S_{n} \leq a$ (lorsque $t<0$)}

\begin{algorithmic}
\STATE Si $t>0$, alors $A \leftarrow [a,\infty)$, sinon $(-\infty,a]$ 
\WHILE{$C \neq 1$}

\STATE Tirer $X_{1},\ldots,X_{n}$ iid selon la densité non-normalisée $e^{tx}f(x)$
\STATE $S_{n} \leftarrow X_{1} + \ldots + X_{n}$
\STATE Tirer $C \sim Bern(\exp(t(a-S_{n}))\mathbf{1}(S_{n}\in A))$ 

\ENDWHILE
\end{algorithmic}
\end{algorithm}


\newpage
\begin{lemme}
Supposons que $mgf_{X_{i}}(t)\exp(-ta/n)<1$. Alors l'algorithme précédent génère une réalisation de $[S_{n}|S_{n}>a]$ lorsque $t>0$ ou de $[S_{n}|S_{n}<a]$ lorsque $t<0$.
\end{lemme}


\begin{proof}
On considère tout vecteur $(x_{1},\ldots,x_{n})$ tel que $\sum x_{i} = s$. On considère $X_{1},\ldots,X_{n}$ iid de densité $f_{X}(x)$ et $X_{1}',\ldots,X_{n}'$ iid de densité $\exp(tx)f_{X}(x)$. Alors la densité de $S_{n}'  = X_{1}' + \ldots + X_{n}'$ est simplement la densité de $S_{n} = X_{1} + \ldots + X_{n}$ avec un facteur $\exp(tx_{1})\exp(tx_{2})\ldots\exp(tx_{n}) = \exp(ts)$.\\
On génère dans l'algorithme une variable aléatoire de densité $g(s) = \exp(ts)f_{S_{n}}(s)$ et la densité ciblée est $f(s) = f_{S_{n}}(s)\mathbf{1}(s \in A)$. On a alors $f(s)/g(s)$ qui est soit $0$ lorsque $s \notin A$ soit $\exp(-ts)$ lorsque $s \in A$.\\
Si $A = [a, \infty)$, alors $t>0$ et $\exp(-ts) \leq \exp(-ta)$. De même, lorsque $A = (-\infty, a]$, alors $t<0$  et $\exp(-ts) \leq \exp(-ta)$. On choisit donc $c = \exp(-ta)$ et on obtient $f(s)/[cg(s)] = \exp(ta)\exp(-ts)\mathbf{1}(s \in A)$ de même que dans l'algorithme. 
\end{proof}

\newpage
\paragraph{\underline{\textbf{Mise en place de la méthode de rejet sour R :}}}
\paragraph{\underline{\textbf{Inégalités de Chernoff et queue de loi binomiale}}}
~\\

\begin{lstlisting}
AR_chernoffs <- function(n,p,a){ ##fonction permettant de générer une réalisation de la loi binomiale de paramètres n et p, conditionnellement à être supérieure où égale à a en utilisant les inégalités de Chernoff##
  
  
  ##On initialise la Bernouilli##
  C=0
  
  ##On créé la valeur gamma présentée précédemment##
  gam = (a/n)*(1-p)/(p*(1-(a/n)))
  
  ##On initialise la Binomiale de paramètres n et p
  t = 0
  
  ##On initialise le nombre de lancers##
  k = 0
  
  ##Tant que la Bernouilli n'est pas 1##
  while (C != 1){
    
    ##On génère n Bernouillis de paramètres a/n## 
    t = rbinom(n,1,a/n)
    
    ##On effectue la somme pour obtenir la binomiale considérée##
    x = sum(t)
    
    ##On génère une réalisation de la Bernouilli du paramètre indiqué
    C = rbinom(1,1,(x>=a)*1*gam**(a-x))
    
    ##On met à jour le nombre de lancers effectués##
    k=k+1
  }
  
  ##On rend soit le nombre de lancers, soit la réalisation de la loi demandée (ie, k ou x))##
  return(k)
  
}


##On calcule ici le nombre moyen de lancers nécessaires pour sortir de l'algorithme précédent

s2=0
for (i in 1:10^5) {   #changer le return pour obtenir le nombre moyen de lancers nécessaires
  s2=s2 + AR_chernoffs(10,0.1,5)
}
s2 = s2/10^5
\end{lstlisting}

\newpage
\begin{lstlisting}
AR_basic <- function(n,p,a){##fonction permettant de générer une réalisation de la loi binomiale de paramètres n et p, conditionnellement à être supérieure où égale à a en utilisant la méthode de rejet basique##
  
  ##On initialise le critère d'arrêt##
  C=0
  
  ##On initialise la Binomiale de paramètres n et p##
  t = 0
  
  ##On initialise le nombre de lancers##
  k = 0
  
  ##Tant que le critère d'arrêt n'est pas 1##
  while (C != 1){
    
    ##On génère une réalisation de la Binomiale de paramètres n et p##
    t = rbinom(1,n,p)
    
    ##On met à jour le critère d'arrêt selon que la binomiale soit supérieure à a ou non##
    C = (t>=a)*1
    
    ##On met à jour le nombre de lancers##
    k=k+1
  }
  
  ##On rend soit le nombre de lancers, soit la réalisation de la loi demandée (ie, k ou t))##
  return(k)
  
}


##On calcule ici le nombre moyen de lancers nécessaires pour sortir de l'algorithme précédent
s1=0
for (i in 1:10^6) {   #changer le return pour obtenir le nombre moyen de lancers nécessaires
  s1=s1+ AR_basic(10,0.1,5)
}
s1= s1/10^6
\end{lstlisting}



\newpage
On utilise l'exemple suivant pour tester nos algorithmes : pour $n=10$ et $p=0.1$, pour $X$ la binomiale de paramètres précisés précédemment, on veut générer une réalisation de $[X|X>=5]$. \\
La méthode de base nous donne un nombre moyen de lancers de $610.9838$ tandis que la méthode à l'aide des inégalités de Chernoff nous donne un nombre moyen de lancers de $3.698136$.\\
D'où l'efficacité de cette méthode.

\subsection{Défaut des méthodes de rejet}

Le principal défaut des méthodes de rejet concerne l'approche des problèmes considérés. On prend pour exemple la génération de la variable aléatoire uniforme dans la boule de dimension $n$.\\
La méthode présentée précédemment dans le cas $n=2$ nécessite de tirer uniformément sur le carré $[-1,1] \times [-1,1]$. La probabilité d'acceptation $p$ est alors l'aire du cercle sur l'aire du carré, ie, $p = \pi /4$.\\
On peut généraliser cette méthode aux dimensions supérieures : on tire uniformément dans l'hypercube unité de dimension $n$ : $[-1,1]^{n}$, le but étant d'obtenir un tirage uniforme dans la boule unité de même dimension. \\
\begin{theorem}[Volume de la boule unité]
Le volume de la boule unité en dimension $n$ est  \Large{$ \frac{\pi^{\frac{n}{2}}}{\Gamma(\frac{n}{2} + 1)} $}\normalsize{, où $\Gamma$ désigne la fonction Gamma.}
\end{theorem}


\begin{proof}
Notons par $V^{(n)}[1]$ le volume la boule unité de dimension $n$ et de rayon $1$. On a d'abord $V^{(1)}[1] = 2$, puis pour tout $n \geq 1$, on a par récurrence, en utilisant la relation suivante : pour $B_{d}(0,1)$ la boule unité de dimension $d$, 

$$B_{d}(0,1) = \left\{ (x_{1}, \ldots, x_{d-1},h) | h \in [-1,1] , (x_{1}, \ldots, x_{d-1}) \in B_{d-1}(0,\sqrt{1 - h^{2}}) \right\}$$ 

Et en utilisant le théorème de Fubini, 

$$ V^{(n+1)}[1] =  \int_{-1}^{1}V^{(n)}[1]\left(\sqrt{1-x^{2}}\right)^{n}dx = V^{(n)}[1]*2\int_{0}^{1}\left(1-x^{2}\right)^{n/2}dx$$

On effectue ensuite le changement de variables $u=x^{2}$ ce qui nous donne $x = \sqrt{u}$ et $dx = \frac{du}{2\sqrt{u}}$ pour obtenir :

$$V^{(n+1)}[1] = V^{(n)}[1]*2\int_{0}^{1}\left(1-x^{2}\right)^{n/2}dx = V^{(n)}[1]\int_{0}^{1}\left(1-u\right)^{n/2} u^{-1/2}du$$

L'intégrale à droite est connue comme la fonction bêta, d'où : 

$$V^{(n+1)}(1) = V^{(n)}[1]B\left(\frac{n}{2}+1,\frac{1}{2}\right)$$

Or on peut exprimer la fonction bêta par rapport à la fonction gamma pour obtenir :

$$V^{(n+1)}(1) = V^{(n)}[1] \frac{\Gamma(\frac{n}{2}+1)\Gamma(\frac{1}{2})}{\Gamma(\frac{n}{2} + \frac{3}{2})}$$

Enfin, en utilisant le fait que $\Gamma(\frac{1}{2}) = \sqrt{\pi}$, et à l'aide d'une simple récurrence, on a enfin que :

\large{}$$V^{(n)}[1]=  \frac{\pi^{\frac{n}{2}}}{\Gamma(\frac{n}{2} + 1)} $$
\normalsize{}

\end{proof}

La probabilité d'acceptation $p$ d'un tirage aléatoire dans l'hypercube unité de dimension $n$ en tant que tirage dans la boule unité de même dimension est donc le volume de la boule sur le volume de l'hypercube, soit encore : 

$$ p = \frac{V^{(n)}[1]}{2^{n}} =  \frac{\pi^{\frac{n}{2}}}{2^{n}\Gamma(\frac{n}{2} + 1)} $$

On en déduit le nombre moyen $k$ de lancers nécessaires à l'acceptation (par simple étude de loi géométrique) : 

$$ k(n) = \frac{1}{p} = \frac{2^{n}\Gamma(\frac{n}{2} + 1)}{\pi^{\frac{n}{2}}}$$

On trace ci-dessous $k$ en fonction de $n$ pour $n \leq 12$ :

\includegraphics[scale=0.93]{graphiques/boule_vitesse.jpeg}

On remarque une augmentation très rapide du nombre de lancers nécessaires à l'obtention d'une réalisation selon que la dimension croît. \\
La méthode de rejet n'est donc plus efficace pour des problèmes dont l'approche n'est pas connue, une étude approfondie étant nécessaire.

\newpage
\section{Chapitre 3 : Coupling From The Past}

\paragraph{Définition : Fonction de mise à jour}
~\\
\\
On dit que $\phi : \Omega \times [0,1] \to \Omega$ est une fonction de mise à jour pour une chaîne de Markov $\left\{X_{t} \right\}$ si, pour $U \sim$ Unif[0,1], $[X_{t+1}|X_{t}] \sim \phi (X_{t},U)$.
\\
La fonction $\phi$ est déterministe : tout l'aléatoire est contenu dans la variable $U$.\\
Toute chaîne qui peut-être simulée sur ordinateur est un exemple de fonction de mise à jour.\\
Une même chaîne de Markov peut-être représentée par plusieurs fonctions de mise à jour : la fonction de mise à jour n'est pas forcément unique.\\
À l'aide d'une fonction de mise à jour $\phi$, on peut représenter la trajectoire d'une chaîne de Markov $\left\{ X_{t} \right\}$. En effet, soient $U_{0},U_{1},U_{2},\ldots$ iid $\sim$ Unif([0,1]). Pour un état initial $x_{0}$, on a : $X_{1} = \phi (x_{0},U_{0})$ puis pour $i>1$, $X_{i} = \phi (X_{i-1},U_{i-1})$. \\ 
On notera alors la trajectoire jusqu'au temps t sous la forme :
$$ \phi_{t}(x_{0},U) = \phi(\phi(\phi(\ldots(\phi(x_{0},U_{0}),U_{1}),\ldots,U_{t-1}))$$
Ensuite, pour n'importe quels états $x_{0}$ et $y_{0}$ dans $\Omega$, on définit pour une fonction de mise à jour $\phi$ :  $X_{t} = \phi_{t}(x_{0},U)$ et $Y_{t} = \phi_{t}(y_{0},U)$ (en utilisant les mêmes valeurs de U). On appelle ce procédé un couplage. Notons qu'avec ce couplage, si il existe $t \geq 0$ tel que $X_{t} = Y_{t}$, alors on dit que les processus ont fusionné (ou se sont rejoints,etc).

\paragraph{Définition : Couplage}
~\\ 
\\
Soit $\mathcal{S}$ un ensemble de processus stochastiques définis sur un même ensemble d'indices $\mathcal{I}$ et un même espace d'états $\Omega$. Si il existe un indice $i \in \mathcal{I}$ et un état $x \in \Omega$ tels que pour tout $S \in \mathcal{S}$, on aie $S_{i} = x$, alors on dit que les processus stochastiques ont coalescé (ou se sont rejoints,couplés,etc). 

\subsection{CFTP : approche trajectorielle (Baccelli / Brémaud)}

Soit $P$ une matrice de transition ergodique sur l'espace d'état fini $E = \left\{ 1,\ldots,r\right\}$, de distribution stationnaire. Comme définie précédemment, à l'aide d'une fonction de mise à jour $h$, on peut implémenter la chaîne de Markov de la manière suivante : 

$$X_{n+1} = h(X_{n},\xi_{n})$$

pour une suite de variables aléatoires $\left\{ \xi_{n} \right\}_{n \geq 1}$ iid uniformes sur $[0,1]$ et indépendantes de l'état initial.\\
On considère à présent un tableau de choix aléatoires $\left\{ \xi_{k}(i) \right\}_{k \in \mathbb{Z}, i \in E}$ iid uniformes sur $[0,1]$. Pour tout $k \in \mathbb{Z}$ et tout $i \in E$, soit $\left\{X_{n}^{k}(i)\right\}_{n \geq k}$ définie par récurrence : 

$$X_{n+1}^{k}(i) = h(X_{n}^{k}(i),\xi_{n}(X_{n}^{k}(i)),~ n \geq k,$$

avec pour condition initiale $X_{k}^{k}(i) = i$, et $h$ comme définie précédemment.\\
Pour tout $k$ et $i$, $\left\{ X_{n}^{k}(i) \right\}_{n \geq k}$ est une chaîne de Markov homogène de matrice de transition $P$. Par la structure de récurrence stochastique sous-jacente, les chaînes de la famille définie précédemment sont telles que : pour tout $k \in \mathbb{Z}$, $X_{n}^{k}(i) = X_{n}^{k}(j)$ implique que $X_{m}^{k}(i) = X_{m}^{k}(j)$, pour tout $m \geq n$.\\
On note par : 

$$ N^{+} = \inf \left\{n \geq 0 | X_{n}^{0}(1) = X_{n}^{0}(2) = \ldots = X_{n}^{0}(r) \right\}$$ 

($= + \infty$ si la condition n'est jamais satisfaite) le temps de coalescence forwards de la chaîne. On notera le temps de coalescence backwards de la chaîne par : 

$$ N^{-} = \inf \left\{n \geq 1 | X_{0}^{-n}(1) = X_{0}^{-n}(2) = \ldots = X_{0}^{-n}(r)\right\} $$

($= + \infty$ si la condition n'est jamais satisfaite)

\begin{center}
\includegraphics[scale=0.9]{graphiques/backwards_coalescence_1.png}
\captionof{figure}{Exemple de temps de coalescence backwards, $N^{-} = 7$}
\end{center}

\begin{center}
\includegraphics[scale=1]{graphiques/forwards_coalescence_1.png}
\captionof{figure}{Exemple de temps de coalescence forwards, $N^{+} = 4$}
\end{center}

\begin{theorem}
Le temps de coalescence forwards $N^{+}$ est presque sûrement fini.
\end{theorem}

\begin{proof}
Il suffit de prouver le résultat dans le cas de $r$ chaînes de Markov homogènes, complètement indépendantes les unes des autres, de même matrice de transition. Nous n'avons pas l'hypothèse d'indépendance dans la construction des chaînes de Markov donnée précédemment. Cependant, la probabilité de coalescence (probabilité que $N^{+}$ soit fini) dans notre situation est bornée inférieurement par la probabilité de coalescence dans le cas complètement indépendant. Pour mieux le comprendre, on construit d'abord le modèle de chaînes indépendantes : 

$$ \overline{X}_{n+1}(i) = h(\overline{X}_{n+1}(i),\overline{\xi}_{n,i}), ~ n \geq 0, $$

(avec pour condition initiale $\overline{X}_{0}(i) = i)$, qui utilise $r$ composantes de mise à jour iid $\left\{ \overline{\xi}_{n,i} \right\}$.

La différence entre ce modèle et celui que l'on a introduit réside dans le nombre de mise à jour trop élevé de notre modèle. Afin de construire un ensemble de $r$ chaînes semblable à celui de notre modèle, il suffit d'utiliser les mêmes mise à jour pour deux chaînes dès lors qu'elles se rencontrent. Il est alors clair que le temps de coalescence forwards du modèle ainsi modifié est plus petit ou égal à celui du modèle complètement indépendant.\\
Il reste alors à prouver que pour un nombre fini de chaînes de Markov homogènes, ergodiques et indépendantes, elles finiront par se rencontrer. Cela suit du fait que le produit de chaînes ergodiques indépendantes est une chaîne ergodique.

\end{proof}

\begin{theorem}
Les variables aléatoires $N^{+}$ et $N^{-}$ ont la même distribution.
\end{theorem}

\begin{proof}
Soit $k \in \mathbb{N}$. On considère le modèle modifié obtenu en remplaçant $\xi_{-k+l}(i)$ par $\xi_{l}(i)$ ,pour tout $l$ tel que $0 \leq l \leq k$ ,et pour $i \in E$.
Notons par $N^{'}$ le temps de coalescence backwards du modèle modifié. Clairement $N^{-}$ et $N^{'}$ ont la même distribution.
\\

\begin{center}
\includegraphics[scale=0.8]{graphiques/forwards_coalescence_2_1}
\includegraphics[scale=0.8]{graphiques/forwards_coalescence_2_2}
\captionof{figure}{$N^{+} \leq k$ implique $N^{'} \leq k$}
\end{center}

On suppose maintenant que $N^{+} \leq k$. Alors, dans le modèle modifié, les chaînes commençant au temps $-k$ depuis les états $1,\ldots,r$ coalescent au temps $-k + N^{+} \leq 0$ (voir figure précédente), et par conséquent $N^{'} \leq k$. Donc, $N^{+} \leq k$ implique $N^{'} \leq k$, puis :

$$\mathbb{P}(N^{+} \leq k) \leq \mathbb{P}(N^{'} \leq k) = \mathbb{P}(N^{-} \leq k)$$

D'autre part, on suppose que $N^{'} \leq k$. Alors, dans le modèle original, les chaînes commençant depuis les états $1, \ldots, r$ au temps $k-N^{'}$ coalesceront au temps $k$. On en déduit donc $N^{+} \leq k$ (voir figure suivante). On a alors que $N^{'} \leq k$ implique $N^{+} \leq k$, puis :

$$\mathbb{P}(N^{-} \leq k) = \mathbb{P}(N^{'} \leq k) \leq \mathbb{P}(N^{+} \leq k)$$

Grâce aux deux inégalités démontrées précédemment, on en déduit le résultat.

\end{proof}

\begin{center}
\includegraphics[scale=0.8]{graphiques/forwards_coalescence_2_4}
\includegraphics[scale=0.8]{graphiques/forwards_coalescence_2_3}
\captionof{figure}{$N^{'} \leq k$ implique $N^{+} \leq k$}
\end{center}

On définit à présent la variable aléatoire suivante : 

$$Z = Z(i) = X_{0}^{-N^{-}}(i)$$

(la variable aléatoire $Z$ est indépendante de $i \in E$; dans la figure 1, on a $Z = 2$).

On a le théorème suivant : 

\begin{theorem}
Le temps de coalescence backwards $N^{-}$ est presque sûrement fini. De plus, la variable aléatoire $Z$ admet $\pi$ comme distribution. 
\end{theorem}

\begin{proof}
Puisque, pour tout $k \in \mathbb{N}$, $\mathbb{P}(N^{-} \leq k) = \mathbb{P}(N^{+} \leq k)$ (d'après le résultat précédent), le fait que $N^{-}$ soit fini provient directement du fait que $N^{+}$ soit fini (théorème 10).\\
D'autre part, puisque pour $n \geq N^{-}$, on a $X_{0}^{-n}(i) = Z$, 

$$\mathbb{P}(Z = j) = \lim_{n \rightarrow \infty } \mathbb{P}(X_{0}^{-n}(i) = j) = \lim_{n \rightarrow \infty } p_{ij}(n) = \pi(j),$$

où la dernière égalité provient du théorème ergodique pour les chaînes de Markov.
\end{proof}

\paragraph{Exemple : distribution stationnaire et temps de coalescence forwards}~\\

Le théorème précédent montre qu'en utilisant le temps de coalescence backwards, on obtient une réalisation de la loi stationnaire par la méthode CFTP. La question est alors la suivante : pourquoi ne pas utiliser le temps de coalescence forwards ? La réponse à cette question s'obtient en étudiant l'exemple simple suivant :\\
On considère $\left\{X_{n}\right\}$ la chaîne de Markov à deux états de matrice de transition 
$P = \left( \begin{matrix}
0 ~~ 1 \\
\frac{1}{2} ~~  \frac{1}{2} 
\end{matrix}\right) $.\\ 
On remarque que cette chaîne a pour loi stationnaire $\pi = (\frac{1}{3} ~ \frac{2}{3})$.\\
 On remarque ensuite que, pour $N^{+}$ le temps de coalescence forwards, on a $X_{0}^{N^{+}}(1)= X_{0}^{N^{+}}(2) = 2$ presque sûrement. En effet, par l'absurde, si les deux chaînes issues des états $1$ et $2$ s'étaient rejointes au temps $N^{+}$ à l'état $1$, alors on aurait deux cas possibles : \\

\begin{itemize}
\item Au temps $N^{+}-1$, la chaîne issue de l'état $1$ se déplace à l'état $1$ au temps suivant : une absurdité par la structure de la chaîne ($P_{1,1} = 0$)
\item Au temps $N^{+}-1$, les deux chaînes ont déjà coalescé, d'où la coalescence au temps $N^{+}$ : une absurdité par la définition de $N^{+}$\\
\end{itemize}

Cependant le temps de coalescence forwards n'est pas une cause perdue : certains modèles permettent d'utiliser la méthode CFTP en conjugaison avec celui-ci pour obtenir une réalisation de la loi stationnaire, comme le montre l'exemple suivant :\\ 

On considère $\left\{X_{n}\right\}$ la chaîne de Markov à deux états de matrice de transition 
$P = \left( \begin{matrix}
\frac{1}{2} ~~  \frac{1}{2}\\
\frac{1}{2} ~~  \frac{1}{2} 
\end{matrix}\right) $.\\ 

On remarque facilement que la loi stationnaire est $\pi = (\frac{1}{2} ~ \frac{1}{2})$, puis que, au temps $N^{+}$ :

$$ \mathbb{P}(X_{0}^{N^{+}}(1) = X_{0}^{N^{+}}(2) = 1) = \mathbb{P}(X_{0}^{N^{+}}(1) = X_{0}^{N^{+}}(2) = 2) = \frac{1}{2}$$

car les deux évènements sont équiprobables et que l'on sait que l'on coalesce forcément pour la première fois au temps $N^{+}$.\\
\newline
La méthode présentée précédemment est particulièrement coûteuse lorsque l'espace d'états est grand (puisqu'il faut faire évoluer des trajectoires à partir de chaque état). Cependant, si la méthode ne requerrait la coalescence que de $2$ chaînes et non plus $r$, celle-ci serait bien plus efficace. Propp et Wilson ont montré comment utiliser cela dans le cas monotone suivant.\\
On suppose qu'il existe une relation d'ordre partiel sur l'espace d'état $E$ que l'on notera par $\preceq$.\\
On suppose ensuite que la fonction de mise à jour préserve cette relation d'ordre, ie, 

$$ i \preceq j \Rightarrow h(i,\xi) \preceq h(j,\xi) ~~ \forall \xi $$

On suppose ensuite que $1  \preceq 2 \preceq \ldots \preceq r$ et on considère le modèle avec une seule mise à jour commune (ie, $\xi_{n}(i) = \xi_{n}$ pour tout $n$ et tout $i$). On définit à présent le temps de coalescence backwards monotone :

$$ M = \inf \left\{n\geq 1 | X_{0}^{-n}(1) = X_{0}^{-n}(r) \right\} $$

($ = + \infty$ si, pour tout $n \geq 1$ la condition n'est pas satisfaite). La procédure de samplage correspondant est appelée algorithme monotone de Propp-Wilson.

\begin{center}
\includegraphics[scale=1]{graphiques/monotonic}
\captionof{figure}{Algorithme monotone de Propp-Wilson; ici $M = 6$}
\end{center}

\begin{theorem}
Le temps de coalescence backwards monotone $M$ est presque sûrement fini. De plus, les variables aléatoires $X_{0}^{-M}(1) = X_{0}^{-M}(r)$ sont de distribution $\pi$.
\end{theorem}	 

\begin{proof}
On appelle $N^{+}$ le temps de couplage forwards des deux chaînes $\left\{ X_{n}^{0}(1) \right\}$ et $\left\{ X_{n}^{0}(r) \right\}$. La preuve que $N^{+}$ est fini est immédiate dans notre cas puisque $N^{+}$ est majoré par le premier temps $n \geq 0$ tel que $X_{n}^{0}(r) = 1$ qui est fini. Ayant cela, la preuve se déroule comme dans celle du théorème $12$ grâce à la propriété de funneling de la chaîne, ie, pour tout $n \in \mathbb{Z}$, pour tout $i \in E$, $X^{k}_{n}(1) \preceq X^{k}_{n}(i) \preceq X^{k}_{n}(r)$ (voir notamment la figure précédente).
\end{proof}

\newpage

\subsection{CFTP : approche par blocs (Huber)}
On considère un état stationnaire $X$. Puis, pour $t$ fixé, soit $Y = \phi_{t}(X,U)$, où $\phi$ est la fonction de mise à jour de la chaîne considérée et $U \in [0,1]^{t}$ sont les choix aléatoires effectués. Alors $Y$ est aussi stationnaire, car c'est la composition de $t$ états stationnaires. \\
La sortie de l'algorithme de CFTP sera toujours $Y$. On considère ensuite $W = (U_{1}, U_{2}, \ldots, U_{t})$ uniforme sur $[0,1]^{t}$ puis un ensemble mesurable $A \subset [0,1]^{t}$. Alors soit $W \in A$, soit $W \notin A$. D'où :
\begin{align}
Y = \phi_{t}(X,W)\mathbf{1}(W \in A) + \phi_{t}(X,W)\mathbf{1}(W \notin A)
\end{align}


Supposons qu'on puisse trouver un ensemble $A$ tel que lorsque $W \in A$, $\phi(X,W)$ ne dépende pas de $X$. C'est à dire, dès lors que $W \in A$, on ait $\phi(x,w) = \phi(x',w)$ pour tout $x,x' \in \Omega$. Une telle chaîne à "oublié" son point de départ lors de son déplacement. Si cela arrive, il n'y a alors pas besoin de connaître la valeur de $X$ pour obtenir $Y$ dans la formule précédente.\\
Pour voir cela, on considère l'exemple suivant :  

\paragraph{Exemple}
~\\
\\
Soit $\Omega = \left\{0,1,2\right\}$. Soit une fonction de mise à jour $\phi$ telle que :
$$ \phi(x,U) = x + \mathbf{1}(x<2,U>\frac{1}{2}) - \mathbf{1}(x>0,U \leq \frac{1}{2}).$$
Soient $\left\{X_{t}\right\}$ et $\left\{Y_{t}\right\}$ deux chaînes de Markov ayant $\phi$ comme fonction de mise à jour et telles que $X_{0} = 0$ et $Y_{0} = 2$. On suppose que $U_{0} = 0.64 , U_{1} = 0.234$ et $U_{2} = 0.1 $. On a donc les trajectoires suivantes : $(X_{0},X_{1},X_{2},X_{3}) = (0,1,0,0)$ et $(Y_{0},Y_{1},Y_{2},Y_{3}) = (2,2,1,0)$.\\
Les deux chaînes ont donc fusionné au temps $t=3$.

Pour cet exemple, on a donc :

$$\phi_{3}(0,W) = \phi_{3}(1,W) = \phi_{3}(2,W) = 0$$

D'où : $\phi_{3}(\left\{0,1,2\right\},W) = \left\{0\right\}$\\
Supposons que les 3 premiers pas étaient (haut,bas,bas). Cette suite de mouvements correspond à l'ensemble des valeurs appartenant à $A_{1} = (\frac{1}{2},1] \times [0,\frac{1}{2}] \times [0,\frac{1}{2}]$. Si $W \in A_{1}$, alors $\phi_{3}(\left\{0,1,2\right\},W) = \left\{ 0 \right\}$.\\
Une autre suite de mouvements valide est (bas,bas,bas) ce qui correspond à $A_{2} = [0,\frac{1}{2}] \times [0,\frac{1}{2}] \times [0,\frac{1}{2}]$. Bien sûr, si $A_{1}$ et $A_{2}$ fonctionnent, alors $A_{3} = A_{1} \cup A_{2}$ fonctionne aussi. D'où $\phi_{3}(\left\{0,1,2\right\},W) = \left\{0\right\}$ pour tout $W \in A_{3}$. \\
Le but ici est de montrer que $A$ n'a pas besoin d'être exactement l'ensemble de tous les mouvements qui coalesceront en un point. Cependant, plus $A$ sera grand, plus la probabilité que $W$ soit dans $A$ sera grande.\\ 	 
Retour à l'équation (2). Dans le premier terme, la valeur de $X$ n'est pas importante, alors on peut écrire : $\phi_{t}(X,W)\mathbf{1}(W \in A) = \phi_{t}(x_{0},W)\mathbf{1}(W \in A)$, où $x_{0}$ est un élément arbitraire de l'espace d'états. On a alors :

$$Y = \phi_{t}(x_{0},W)\mathbf{1}(W \in A) + \phi_{t}(X,W)\mathbf{1}(W \notin A)$$

Donc lorsque $W \in A$, il n'y a pas besoin de connaître la valeur de $X$ afin de calculer $Y$. Cependant, lorsque $W \notin A$, on doit évaluer $\phi_{t}(X,W)$ pour obtenir $Y$. L'idée principale du CFTP est de trouver $X \sim \pi$ en appelant récursivement le CFTP (c'est la partie "from the past" de l'algorithme) puis en calculant $Y$ comme précédemment. Alors par définition de $Y$, on générera éventuellement un $W \in A$ tel qu'on ait pas besoin de connaître $X$, et il ne restera plus qu'à dérouler tous les appels récursifs effectués. \\
Pour comprendre cela en pratique, on altère légèrement notre notation en ajoutant un indice de temps. Soit $Y_{0} = Y$, $W_{0} = W$, et $Y_{-1} = W$. Avec cette notation : 

$$ Y_{0} = \phi_{t}(x_{0}, W_{0})\mathbf{1}(W_{0} \in A) + \phi_{t}(Y_{-1},W_{0})\mathbf{1}(W_{0} \notin A)$$

Donc si $W_{0} \in A$, on est bon, sinon, on doit générer $Y_{-1}$. Ce sera fait de la même manière que lorsque l'on à généré $Y_{0}$ : 

$$ Y_{-1} = \phi_{t}(x_{0}, W_{-1})\mathbf{1}(W_{-1} \in A) + \phi_{t}(Y_{-2},W_{1})\mathbf{1}(W_{1} \notin A)$$

où $W_{-1} \sim Unif([0,1])^{t}$ et est indépendant de $W_{0}$.\\
En général, 
$$ Y_{-1} = \phi_{t}(x_{0}, W_{-i})\mathbf{1}(W_{-i} \in A) + \phi_{t}(Y_{-i-1},W_{-i})\mathbf{1}(W_{-i} \notin A)$$

\begin{theorem}
Théorème CFTP (Coupling From The Past)
\end{theorem}

On suppose que $\phi$ est une fonction de mise à jour pour une chaîne de Markov définie sur $\Omega$, telle que pour $U = (U_{0},U_{-1},\ldots,U_{-t-1}) \sim$ Unif($[0,1]^{t}$) on ait :\\
\begin{itemize}
\item Pour $Y \sim \pi, \phi (Y,U_{0}) \sim \pi$
\item Il existe un ensemble $A \subseteq [0,1]^{t}$ tel que $\mathbb{P}(U \in A) > 0$ et $\phi_{t}(\Omega,A) = \left\{ x \right\}$ pour un certain $x \in \Omega$
\end{itemize}
Posons alors, pour tout $x \in \Omega$,

\begin{align*}
Y_{0} = \phi_{t}(x_{0},U_{0})\mathbf{1}(U_{0} \in A) + \phi_{t}(\phi_{t}(x_{0},U_{-1}),U_{0})\mathbf{1}(U_{-1} \in A) 
&+ \\ \phi_{t}(\phi_{t}(\phi_{t}(x_{0},U_{-2}),U_{-1}),U_{0})\mathbf{1}(U_{-2} \in A) + \ldots 
\end{align*}
 

Alors $Y_{0} \sim \pi$.

\begin{proof}
Soit $x_{0} \in \Omega$. Le résultat est immédiat en utilisant le théorème fondamental de la simulation parfaite, en posant $g(U) = \phi_{t}(x_{0},U)$,$b(U) = \mathbf{1}(U \in A)$, et $f(X,U) = \phi_{t}(X,U)$.
\end{proof}

L'idée clé qu'ont eu Propp et Wilson est qu'il n'est pas nécessaire de connaître tous les termes de la suite pour trouver $Y_{0}$. Il suffit en effet de connaître $U_{0},\ldots,U_{-T}$, où $U_{T} \in A$. Tant que $\mathbb{P}(U \in A) > 0$, alors $T$ suivra une loi géométrique de paramètre $\mathbb{P}(U \in A)$. Le pseudo-code suivant accomplit alors cette tâche : 


\floatname{algorithm}{Coupling-from-the-past}
\begin{algorithm}
\caption{Sortie : $Y \sim \pi$ }
\begin{algorithmic}[1]
\STATE Tirer $U \sim Unif([0,1]^{t})$
\IF{$U \in A$}
\STATE Rendre $\phi_{t}(x_{0},U)$ (où $x_{0}$ est un élément arbitraire de $\Omega$)
\ELSE
\STATE $X \leftarrow $ Coupling-from-the-past
\STATE Rendre $\phi_{t}(X,U)$
\ENDIF
\end{algorithmic}
\end{algorithm}

Cet algorithme est un algorithme récursif, autorisé à s'appeler à nouveau en ligne $5$. Lorsque cela arrive, les valeurs de $U$ ne sont pas passées en paramètre : le nouvel appel génère en ligne $1$ ses propres valeurs de $U$ indépendantes des précédentes. C'est une étape importante, car sinon le théorème fondamental de la simulation parfaite  ne s'appliquerait pas.\\
On considère à présent le temps d'exécution de l'algorithme mesuré par le nombre d'appels à $\phi$.

\begin{lemme}
Le nombre d'appels à $\phi$ dans l'algorithme Coupling-from-the-past est en moyenne \large{}$\frac{t}{\mathbb{P}(U \in A)}$.\normalsize{}
\end{lemme}

\begin{proof}
Soit $T = \inf\left\{\tau \geq 1 | U_{\tau}^{t} \in A\right\}$. $T$ suit par définition la loi géométrique de paramètre $\mathbb{P}(U \in A)$

Alors, l'algorithme s'arrête au temps $T$ après être passé $T-1$ fois dans la partie ELSE puis une fois dans la première partie. \\
Dans la partie ELSE, l'algorithme fait appel $t$ fois à $\phi$, d'où : $(T-1)*t$ appels à $\phi$.\\
La première partie fait aussi $t$ appels à $\phi$. En sommant, on obtient le nombre total d'appels : $T*t$.\\
Or, on connaît la loi de $T$, donc, en prenant l'espérance, on obtient :

$$ \mathbb{E}[T*t] = t*\mathbb{E}[T] = \frac{t}{\mathbb{P}(U \in A)} $$

\end{proof}

Pour garder le temps d'exécution suffisamment bas, il est important que $t$ soit suffisamment grand pour que $\mathbb{P}(U \in A)$ soit raisonnablement élevé. Dès lors que cette condition est achevée, $t$ devra être le plus petit possible, afin de garder un nombre de mise à jour faible.\\
Cependant, il existe un moyen de contourner ce problème.

\subsubsection*{Variation de la taille des blocs}

Lors de la construction du CFTP, une fonction de mise à jour à été composée avec elle-même $t$ fois. On appelle cet ensemble de $t$ pas un bloc.\\
On suppose qu'il y ait un seuil $t'$ tel que $\mathbb{P}(U \in A) = 0$ si $t < t'$. En pratique, un tel seuil est très difficile à calculer : il est donc commun d'utiliser des blocs de taille qui augmente afin d'être sûr que $t$ dépassera éventuellement ce seuil.\\
En général, il n'y a aucune raison pour que le même nombre de pas soit utilisé pour chaque bloc.\\
Le théorème fondamental de la simulation parfaite s'adapte facilement au cas où $b,g$ et $f$ changent à chaque étape de la récursion.

\begin{theorem}[Théorème fondamental de la simulation parfaite (seconde forme)]
On suppose que pour $U_{1},U_{2},\ldots$ iid, on ait des suites de fonctions calculables $\left\{b_{t} \right\}$,$\left\{g_{t} \right\}$ et $\left\{f_{t} \right\}$ telles que chaque $\left\{b_{t} \right\}$ ait pour image $\left\{0,1\right\}$ et que $\prod_{t=1}^{\infty} \mathbb{P}(b_{t}(U) =0) =0$. Soit $X$ la variable aléatoire telle que, pour tout $t$ :

$$ X \sim b_{t}(U)g_{t}(U) + (1-b_{t}(U))f_{t}(X,U)$$

Soit $T = \inf\left\{t : b_{t}(U_{t}) = 1 \right\}$. Alors, 

$$ Y = f_{0}(\ldots f_{T-2}(f_{T-1}(g_{T}(U_{T}),U_{T}),U_{T-1}),\ldots,U_{1})$$

a la même distribution que $X$ et $\mathbb{E}[T] = \frac{1}{\mathbb{P}(b_{t}(U_{t}) = 1)}$

\end{theorem}

\begin{proof}
La preuve est presque identique à celle du théorème fondamental de la simulation parfaite présentée dans le chapitre $1$.
\end{proof}

On utilise cette forme du théorème fondamental de la simulation parfaite pour obtenir le résultat suivant :

\begin{theorem}
Soit $t(0) < t(1) < t(2) < \ldots$ des entiers positifs. Soit $W_{0},W_{1},\ldots$ indépendants tels que $W_{-i} \sim Unif([0,1]^{t(i)})$, et $A_{t(i)}$ un ensemble mesurable de $[0,1]^{t(i)}$ tel que $\phi_{t(i)}(\Omega,W_{-i}) = \left\{ y \right\}$. On suppose que $\prod_{i=1}^{\infty} \mathbb{P}(W_{-i} \in A) = 0$. Alors pour tout $x_{0} \in \Omega$,

\begin{align*}
Y_{0} &= \phi_{t(0)}(x_{0},W_{0})\mathbf{1}(W_{0} \in A_{t(0)}) + \phi_{t(0)}(\phi_{t(1)}(x_{0},W_{-1}),W_{0})\mathbf{1}(W_{-1} \in A_{t(1)})\\
&+ \phi_{t(0)}(\phi_{t(1)}(\phi_{t(2)}(x_{0},W_{-2}),W_{-1}),W_{0})\mathbf{1}(W_{-2} \in A_{t(2)}) + \ldots
\end{align*}

est tel que $Y_{0} \sim \pi$.
 
\end{theorem}

Un choix usuel est de poser $t(i) = 2*t(i-1)$ afin que le nombre de pas double à chaque étape de l'algorithme. Si le $t$ initial est $t(0) = 1$, alors doubler le nombre de pas à chaque itération nous permettra d'atteindre $t'$ après un nombre logarithmique de récursions. De plus le travail total effectué dans tous les blocs sera de $1 + 2 + 4 + \ldots + t' = 2*t' - 1$. On utilise ce choix de $t(i)$ pour obtenir l'algorithme suivant :

\floatname{algorithm}{Doubling-Coupling-from-the-past}
\begin{algorithm}
\caption{Entrée : t, Sortie : $Y \sim \pi$ }
\begin{algorithmic}[1]
\STATE Tirer $U \sim Unif([0,1]^{t})$
\IF{$U \in A_{t}$}
\STATE Rendre $\phi_{t}(x_{0},U)$ (où $x_{0}$ est un élément arbitraire de $\Omega$)
\ELSE
\STATE $X \leftarrow $ Doubling-Coupling-from-the-past($2*t$)
\STATE Rendre $\phi_{t}(X,U)$
\ENDIF
\end{algorithmic}
\end{algorithm}
 
On notera que le choix $t(i) = 2*t(i-1)$ n'est pas mandatoire, d'autres choix sont valides. Seul le nombre de pas pris par l'algorithme (en moyenne) sera affecté.

\subsection{CFTP monotone}

La partie difficile permettant d'utiliser le CFTP est de créer les ensembles $A_{t(i)}$ ainsi qu'une méthode permettant de déterminer si $U \in A_{t(i)}$.\\
C'est cette difficulté qui empêche d'utiliser le CFTP dans toutes les applications de Monte Carlo.\\
On présentera ici une méthode permettant de trouver de tels $A_{t(i)}$ dans différentes applications.\\
L'idée est la suivante. On suppose que $\Omega$ est un espace d'états admettant un ordre partiel (voir définition $11$).\\
Comme exemple, on considère le modèle d'Ising et deux configurations $x$ et $y$. On dit  que $x \preceq y$ si pour tout n\oe{}ud $\nu$ on a $x(\nu) \leq y(\nu)$. Par exemple, si on a $4$ n\oe{}uds, $ (-1,-1,-1,1) \preceq (-1,-1,1,1)$. D'autre part, les configurations $(-1,-1,1,1)$ et $(1,1,-1,-1)$ ne sont pas comparables.\\
Ensuite, on supposera que $\Omega$ est doté d'un plus grand élément $x_{max}$, et d'un plus petit élément $x_{min}$. Pour continuer avec l'exemple du modèle d'Ising, la configuration n'ayant que des $-1$ est le plus petit élément, celle n'ayant que des $1$, le plus grand.

\begin{definition}
Fonction de mise à jour monotone
\end{definition}

Soit $\phi$ une fonction de mise à jour pour un pas d'une chaîne de Markov. Si $\phi$ est telle que, pour tout $x preceq y$, et $u \in [0,1]$, on ait $\phi(x,u) \preceq \phi(y,u)$, alors on dit que $\phi$ est monotone.\\
~\\

On considère à présent deux chaînes de Markov $\left\{X_{t} \right\}$ et $\left\{Y_{t} \right\}$ telles que $X_{0} = x_{0}$ et $Y_{0} = y_{0}$ où $x_{0} \preceq y_{0}$. On suppose qu'à chaque pas, $X_{t+1} = \phi(X_{t},U_{t})$ et $Y_{t+1} = \phi(Y_{t},U_{t})$ où $U_{1},U_{2},\ldots$ sont iid telles que $U_{1} \sim Unif([0,1])$.\\
On en déduit simplement que $X_{t+1} \preceq Y_{t+1}$ pour tout $t$.\\
On suppose de plus que $x_{0} = x_{min}$ et $y_{0} = x_{max}$. Alors tout autre état $w_{0}$ vérifie $x_{0} \preceq w_{0} \preceq y_{0}$ et on construit alors de manière similaire une chaîne de Markov $W_{t}$ qui donnera $X_{t} \preceq W_{t} \preceq Y_{t}$ pour tout $t$.\\
On suppose maintenant que $X_{t} = Y_{t}$. Les propriétés d'ordre partiel impliquent alors que $X_{t} = W_{t} = Y_{t}$. L'état de départ de $W_{t}$ était arbitraire, donc pour tout état de départ $w_{0} \in \Omega$, $\phi_{t}(w_{0},u) = X_{t} = Y_{t}$\\
En d'autres termes, si après un nombre fixé de pas, les états extrémaux ont atteint le même état, alors tous les autres états ont été "piégés" entre ceux-ci et ont donc aussi atteint le même état.\\
On obtient alors une variante importante du CFTP : le CFTP monotone.

\floatname{algorithm}{Monotonic-Coupling-from-the-past}
\begin{algorithm}
\caption{Entrée : t Sortie : $Y$}
\begin{algorithmic}[1]
\STATE Tirer $U \sim Unif([0,1]^{t})$
\STATE Soit $X_{t} \leftarrow \phi_{t}(x_{max},U)$ et $Y_{t} \leftarrow \phi_{t}(x_{min},U)$
\IF{$X_{t} = Y_{t}$}
\STATE Rendre $X_{t}$
\ELSE
\STATE Tirer $X \leftarrow $ Monotonic-Coupling-from-the-past($2*t$)
\STATE Rendre $\phi_{t}(X,U)$
\ENDIF
\end{algorithmic}
\end{algorithm}

\subsubsection{Modèle D'Ising}

On considère la mise à jour de Gibbs pour le modèle d'Ising de la section $1.4.1$.\\
Lors de la mise à jour, un n\oe{}ud $\nu$ était choisi uniformément aléatoirement et $U \sim Unif([0,1])$. On note $N_{c}$ le nombre de voisins de $\nu$ de label $c$. Si $U < \exp(N_{1}\beta)/[\exp(N_{1}\beta + \exp(N_{-1}\beta)]$, alors le n\oe{}ud devenait de label $1$, sinon de label $-1$.\\
Afin de pouvoir utiliser cette mise à jour pour la simulation parfaite, on doit l'écrire comme fonction de mise à jour, c'est-à-dire, l'état actuel et les choix aléatoires doivent être des paramètres d'entrée.

\newpage
\floatname{algorithm}{Ising-Gibbs}
\begin{algorithm}
\caption{Entrée : $x \in \left\{ -1,1 \right\}^{V}, u \in [0,1], \nu \in V$ Sortie : $x \in \left\{ -1,1 \right\}^{V} $}
\begin{algorithmic}[1]
\STATE $N_{1} \leftarrow$ nombre de voisins de $\nu$ de label $1$ dans $x$
\STATE $N_{-1} \leftarrow$ nombre de voisins de $\nu$ de label $-1$ dans $x$
\IF{$U < \exp(N_{1}\beta)/[\exp(N_{1}\beta + \exp(N_{-1}\beta)]$}
\STATE $x(\nu) \leftarrow 1$
\ELSE
\STATE $x(\nu) \leftarrow -1$
\ENDIF
\end{algorithmic}
\end{algorithm}

\begin{lemme}
La fonction créée précédemment est monotone lorsque $\beta > 0$
\end{lemme}

\begin{proof}
Soit $x \preceq y$. Soit $\nu$ un n\oe{}ud du graphe considéré et la fonction $N_{c}(z)$ notant le nombre de voisins de $\nu$ de label $c$ dans la configuration $z$. Alors tout voisin de $\nu$ dans $x$ de label $1$ est aussi de label $1$ dans $y$, donc $N_{1}(x) \leq N_{1}(y)$. De manière similaire, on a $N_{-1}(x) \leq N_{-1}(y)$.\\
Soit $f(n_{1},n_{-1}) = \exp(N_{1}(x)\beta)/[\exp(N_{1}(x)\beta + \exp(N_{-1}\beta)]$. Puisque $\beta >0$, $f(n_{1},n_{-1})$ est croissant en $n_{1}$ et décroissant en $n_{-1}$. Donc si $U < f(N_{1}(x),N_{-1}(x))$ alors on aura $U < f(N_{1}(y),N_{-1}(y))$. C'est-à-dire, si $x(\nu)$ est changé en $1$, alors $y(\nu)$ aussi.\\
De même, si $U > f(N_{1}(y),N_{-1}(y))$, alors $U > f(N_{1}(x),N_{-1}(x))$. C'est-à-dire, si $y(\nu)$ devient $0$, alors $x(\nu)$ aussi.\\
Enfin, si $f(N_{1}(x),N_{-1}(x)) < U < f(N_{1}(y),N_{-1}(y))$, alors seul le n\oe{}ud $x(\nu)$ est changé en $-1$, $y(\nu)$ devient $1$, l'ordre n'est alors pas changé.\\
Donc, peu importe le choix de $\nu$ et $U$ dans la mise à jour, si $x \preceq y$, alors $\phi(x,\nu,U) \preceq \phi(y,\nu,U)$.
\end{proof}

Cela signifie que le CFTP monotne peut-être utilisé pour générer un échantillon aléatoire de la distribution stationnaire.
\newpage
\floatname{algorithm}{Monotonic-Ising-Gibbs}
\begin{algorithm}
\caption{Entrée : $t$ Sortie :$X \sim \pi$}
\begin{algorithmic}[1]
\STATE Tirer $(U_{1},\ldots,U_{t}) \leftarrow Unif([0,1]^{t})$
\STATE Tirer $(V_{1},\ldots,V_{t}) \leftarrow Unif(V^{t})$
\STATE $x_{max} \leftarrow (1,\ldots,1)$, $x_{min} \leftarrow (0,\ldots,0)$
\FOR{i allant de $1$ à $T$}
\STATE $x_{max} \leftarrow$ Ising-Gibbs($x_{max}$,$U_{i}$,$V_{i}$)
\STATE $x_{min} \leftarrow$ Ising-Gibbs($x_{min}$,$U_{i}$,$V_{i}$)
\ENDFOR
\IF{$x_{max} \ne x_{min}$}
\STATE $x_{max} \leftarrow$ Monotonic-Ising-Gibbs($2*t$)
\FOR{i allant de $1$ à $t$}
\STATE $x_{max} \leftarrow$ Ising-Gibbs($x_{max}$,$U_{i}$,$V_{i}$) 
\ENDFOR
\ENDIF
\STATE $X \leftarrow x_{max}$
\end{algorithmic}
\end{algorithm}

\subsubsection{Modèle gas Hard-core sur graphe bipartite}

On considère maintenant la mise-à-jour HCGM-Gibbs de la section $1.4.4$. Cette fonction de mise-à-jour n'est pas monotone en utilisant l'ordre partiel $x \preceq y \leftrightarrow (\forall \nu \in V)(x(\nu) \leq y(\nu))$.\\
Il est alors nécessaire de trouver une autre fonction de mise-à-jour ou un autre ordre partiel. Le problème est que la chaîne est répulsive,ie, deux n\oe{}uds voisins ne peuvent avoir le label $1$. Cependant, si le graphe est biparti, il est possible de construire un nouvel ordre partiel tel que l'on ait une fonction de mise-à-jour monotone.

\begin{definition}
Graphe biparti
\end{definition}

On dit qu'un graphe est biparti lorsque l'ensemble des n\oe{}uds $V$ peut être partitionné en deux ensembles $V_{1}$ et $V_{2}$ tels que pour toute arête $e$, un n\oe{}ud appartienne à $V_{1}$ et l'autre à $V_{2}$.\\
Pour les configurations sur graphe biparti, on utilise l'ordre partiel suivant (similaire à l'ordre partiel défini précédemment) :

$$x \preceq y \leftrightarrow (\forall \nu_{1} \in V_{1})(\forall \nu_{2} \in V_{2}) (x(\nu_{1}) \leq y(\nu_{1}) \land x(\nu_{2}) \geq y(\nu_{2}))$$

\begin{lemme}
HCGM-Gibbs est monotone relativement à l'ordre partiel ci-dessus
\end{lemme}

\begin{proof}
Si $x \preceq y$ et le n\oe{}ud $i \in V_{2}$ est choisi pour être mis-à-jour, alors si un voisin $w$ de $i$ est tel que $x(w) = 1$, alors $y(w) = 1$ aussi. Donc $x(i)$ et $y(i)$ resteront tous deux à $0$. La seule manière pour que $y(i)$ soit changé en $1$ est si $y(w) = 0$ pour tout voisin $w$ de $i$. Dans ce cas, tout voisin vérifie aussi $x(w) = 0$. D'où : $x(i) = y(i) = \mathbf{1}(U < \lambda / (\lambda + 1))$.\\
L'analyse pour $i \in V_{1}$ est similaire.
\end{proof}

L'état "le plus grand" est alors celui dont tous les n\oe{}uds de $V_{2}$ sont de label $1$ et tous les n\oe{}uds de $V_{1}$ sont de label $0$.\\ L'état "le plus petit" voit ses labels inversés par rapport au plus grand.

\subsubsection{Monotonie et temps de mélange}

Une des raisons pour lesquelles la monotonie est importante est que le CFTP monotone ne prendra pas plus de temps à fonctionner  
que le temps de mélange de la chaîne de Markov.

\begin{definition}
Chaîne en ordre partiel
\end{definition}

Une chaîne de longueur $l$ en ordre partiel est un sous-ensemble de $l+1$ éléments tels que $ x_{0} \prec x_{1} \prec x_{2} \prec \ldots \prec x_{l}$\\
~\\
On présente ensuite une propriété importante de la distance en variation totale : celle-ci permet la représentation de deux variables aléatoires en utilisant une troisième variable aléatoire commune.

\begin{lemme}
On suppose que $d_{TV}(X,Y) = d$, où $X,Y$ sont des variables aléatoires de densités respectives $f_{X}$ et $f_{Y}$ par rapport à la mesure $\mu$ sur $\Omega$. Alors il existe des variables aléatoires $W_{1},W_{2},W_{3}$ et $B \sim Bern(d)$ indépendantes telles que :

$$ X \sim (1-B)W_{1} + BW_{2}$$
$$ Y \sim (1-B)W_{1} + BW_{3}$$
\end{lemme}

\begin{proof}
On procède ici par disjonction de cas sur $d$.

\begin{itemize}
\item $d=0$. Alors les variables aléatoires $X$ et $Y$ sont égales, on pose $W_{1} = X (= Y)$ et $W_{2},W_{3}$ ont n'importe quelle distribution (car $B = 0$).

\item $d=1$. Il suffit alors de poser $W_{2} = X$ et $W_{3} = Y$ puisque $B = 1$.

\item $0 < d < 1$. On pose alors $g(s) = \min(f_{X}(s),f_{Y}(s))$ puis à $W_{1}$ on donne la densité $g/\int_{\Omega}g d\nu$,à $W_{2}$ la densité $[f_{X}(s) - g(s)]/\int_{\Omega}(f_{X}(r) - g(r))d\nu(r)$ et à $W_{3}$ la densité $[f_{Y}(s) - g(s)]/\int_{\Omega}(f_{Y}(r) - g(r))d\nu(r)$.\\
Il reste à voir maintenant que $\int_{\Omega}g d\nu = 1 - d$. Pour le montrer, on rappelle d'abord que la distance en variation totale est $d = \sup_{D} |\mathbb{P}(Y \in D) - \mathbb{P}(X \in D)|$. Puisque $|\mathbb{P}(Y \in D) - \mathbb{P}(X \in D)| = |\mathbb{P}(Y \in D^{C}) - \mathbb{P}(X \in D^{C})|$, le supremum peut être considéré sur les ensembles $D$ tels que $\mathbb{P}(Y \in D) \geq \mathbb{P}(X \in D)$.\\
Soit à présent $C = \left\{ s \in \Omega | f_{X}(s) \leq f_{Y}(s) \right\}$. Soit $D$ un ensemble mesurable tel que $\mathbb{P}(Y \in D) \geq \mathbb{P}(X \in D)$. On a, pour toute variable aléatoire $W$, $\mathbb{P}(W \in D) = \mathbb{P}(W \in C) - \mathbb{P}(W \in C \cap D^{C}) + \mathbb{P}(W \in C^{C} \cap D)$.\\
Pour voir cela, il suffit de voir que, pour une mesure $\mu$ et des ensembles mesurables $C$ et $D$ :

$$\mu(D \cap C) = \mu(C \setminus (D^{C} \cap C)) = \mu(D \setminus (D \cap C^{C}))$$

puis que, 
$$\mu(C) - \mu(D^{C} \cap C) = \mu(D) - \mu(D \cap C^{C})$$

On définit ensuite, pour toute variable aléatoire $W$, $h(W) = \mathbb{P}(W \in C \cap D^{C}) - \mathbb{P}(W \in C^{C} \cap D)$. On a alors : 

$$\mathbb{P}(Y \in D) - \mathbb{P}(X \in D) = \mathbb{P}(Y \in C) - \mathbb{P}(X \in C) - h(Y) + h(X)$$

Or, 

$$-h(Y) + h(X) = [\mathbb{P}(X \in C \cap D^{C}) - \mathbb{P}(Y \in C \cap D^{C})] + [\mathbb{P}(Y \in C^{C} \cap D) - \mathbb{P}(X \in C^{C} \cap D)]$$

De la manière dont $C$ a été choisi, les deux termes entre crochets sont non-positifs ou nuls, on a donc :

$$ 0 \leq \mathbb{P}(Y \in D) - \mathbb{P}(X \in D) \leq \mathbb{P}(Y \in C) - \mathbb{P}(X \in C)$$

Puisque $D$ était un ensemble arbitraire tel que $\mathbb{P}(Y \in D) \leq \mathbb{P}(X \in D)$, la distance de variation totale est alors $d = \mathbb{P}(Y \in C) - \mathbb{P}(X \in C)$.\\
On considère à présent la constante de normalisation de $W_{2}$.

\begin{align*}
\int_{\Omega}f_{X}(s) - g(s) d\nu(s) &= \int_{C} f_{X}(s) - g(s)d\nu(s) + \int_{C^{C}} f_{X}(s) - g(s)d\nu(s)\\
&= 0 + \int_{C^{C}} f_{X}(s) - f_{Y}(s)d\nu(s) \\
&= \mathbb{P}(X \in C^{C}) - \mathbb{P}(Y \in C^{C}) = \mathbb{P}(Y \in C^{C}) - \mathbb{P}(X \in C^{C}) = d
\end{align*}
La densité de $(1-B)W_{1} + BW_{2}$ est : 

$$(1-d)\frac{\min(f_{X}(s),f_{Y}(s))}{1-d} + d\frac{f_{X} - \min(f_{X}(s)(s),f_{Y}(s))}{d} = f_{X}(s)$$

la même densité que $X$. Un résultat similaire vaut pour $Y$.


\end{itemize}

\end{proof}

On se sert de ce résultat dans le théorème suivant :\\

\begin{theorem}[Propp et Wilson]
Pour un ensemble discret $\Omega$, soit $l$ la longueur de la chaîne la plus longue en ordre partiel de $\Omega$. Soit $p$ la probabilité qu'un appel à Monotonic-coupling-from-the-past($t$). Notons $d_{t} = \sup_{x \in \Omega} d_{TV}([X_{t} | X_{0} = x], \pi)$. On a alors :

$$ \frac{p}{l} \leq d_{t} \leq p$$

\end{theorem} 

\begin{proof}
Le fait que $d_{t} \leq p$ est provient du Lemme de couplage (voir chapitre $1$ théorème 4).\\
On va montrer ici que $\frac{p}{l} \leq d_{t}$. Pour un élément $x \in \Omega$, soit $h(x)$ la longueur de la chaîne la plus longue ayant pour plus grand élément $x$. Si $X_{t}$ est la chaîne commençant en $x_{min}$, et $Y_{t}$ celle commençant en $x_{max}$, alors on aura toujours $X_{t} \preceq Y_{t}$. On notera que si $X_{t} \prec Y_{t}$, alors $h(X_{t}) + 1 \leq h(Y_{t})$, mais si $X_{t} = Y_{t}$ alors $h(X_{t}) = h(Y_{t})$. On en conclut donc que $h(Y_{t}) - h(X_{t})$ est une variable aléatoire entière positive ou nulle qui vaut au moins $1$ lorsque $X_{t} \neq Y_{t}$. On obtient, d'après l'inégalité de Markov : 

$$p = \mathbb{P}(X_{t} \neq Y_{t}) \leq \mathbb{E}[h(Y_{t}) - h(X_{t})]$$

En utilisant le lemme $12$, on sait qu'il existe $W_{1},W_{2},W_{3}$ et $B \sim Bern(d_{t})$ indépendants tels que $X_{t} \sim (1-B)W_{1} + BW_{2}$ et $Y_{t} \sim (1-B)W_{1} + BW_{3}$. D'où : 

$$p \leq \mathbb{E}[h(Y_{t}) - h(X_{t})] = \mathbb{E}[B(h(W_{2}) - h(W_{3}))] = d_{t}l$$
\end{proof}

La distance de variation totale tend à décroître très rapidement car c'est un exemple de fonction sous-multiplicative.

\begin{lemme}
La distance de variation totale $d_{t}$ vérifie $d_{t+s} \leq d_{t}d_{s}$.
\end{lemme}

La preuve comprend l'utilisation du lemme de couplage cité dans le théorème $17$. La propriété énoncée permet de montrer que la taille d'un bloc dans le CFTP monotone nécessaire à la coalescence n'est pas plus grand que le temps que met la chaîne de Markov à commencer à se mélanger.

\begin{lemme}
On suppose que $d_{t} \leq e^{-1}$. Alors pour un bloc de taille $t[k+ln(l)]$, la probabilité $p$ qu'un bloc ne coalesce pas est au plus $e^{-k}$.
\end{lemme}

\begin{proof}
Par la propriété de sous-multiplicativité, $d_{t[k + ln(k)]} \leq [e^{-1}]^{k+ln(l)} = l^{-1}e^{-k}$. En utilisant le théorème précédent cela donne : $p \leq ll^{-1}e^{-k} = e^{-k}$.
\end{proof}

\begin{lemme}
On suppose que dans le CFTP monotone lancé pour un bloc de taille $t$, on ait $\mathbb{P}(X_{t} \ne Y_{t}) \leq e^{-1}$. Alors le nombre moyen de pas de la chaîne de Markov utilisé par Monotonic-Coupling-from-the-past($t$) est au plus de $19.2t$.
\end{lemme}

\begin{proof}
Un appel à Monotonic-Coupling-from-the-past avec pour entrée $s$ requiert au plus $2s$ pas de la chaîne de Markov : $s$ pas pour déterminer si $X_{t} = Y_{t}$ puis $s$ autres pas après l'appel récursif si $X_{t} = Y_{t}$\\
Notons $R_{k}$ l'évènement suivant : le $k$ème appel à Monotonic-Coupling-from-the-past ne vérifie pas $X_{t} = Y_{t}$. Si le premier appel utilise un seul pas et que le nombre de pas double à chaque appel, alors après $\lceil log_{2}(t) \rceil$ appels, le prochain appel a un paramètre d'au moins $t$, et n'échoue donc qu'avec probabilité au plus $e^{-1}$. D'où : 

\begin{align*}
\mathbb{E}[2S] &= \sum_{k=1}^{\infty} 2*2^{k}\mathbb{P}(R_{1}R_{2}\ldots R_{k-1})\\
&\leq 2*(2t) + \sum_{k= \lceil log_{2}(t) \rceil +1}^{\infty}2^{k+1}(e^{-1})^{k-1-\lceil log_{2}(t) \rceil} \\
&\leq 4t + 4t/(1-2/e) \leq 19.2t \\
\end{align*}

\end{proof}

Comme exemple, considérons le modèle d'Ising. La taille maximale d'une chaîne est $\#V$, le nombre de n\oe{}uds du graphe. Donc, si $t$ est le temps nécessaire pour que la distance de variation totale descende en dessous de $e^{-1}$, alors $O(tln(\#V))$ pas sont nécessaires en moyenne pour générer une simulation exacte du modèle. D'autre part, o($t$) pas sont aussi nécessaires pour générer une sortie avec la simulation exacte : le CFTP ne fait donc pas converger la chaîne de Markov plus vite qu'il ne faut.\\
La partie importante du CFTP ici est que la connaissance du $t$ pour lequel la distance de variation totale est au plus $e^{-1}$ n'est pas nécessaire pour utiliser la procédure. Lorsque l'algorithme s'arrête, le résultat est garanti d'avoir été généré selon la distribution ciblée.


\subsection{Slice sampler}

(plus tard)

\subsection{Inconvénients du CFTP}

Bien que le CFTP soit un outil puissant pour créer des algorithmes de simulation exacte, il a quand même des défauts. Les deux principaux problèmes sont la non-interruptibilité et la lecture double.

\subsubsection{Non-interruptibilité}

Considérons d'abord un exemple. On veut générer une loi géométrique à partir de $B_{1},B_{2}, \ldots$ des réalisations de $Bern(p)$, puis en posant $T = \inf \left\{ t | B_{t} = 1 \right\}$ et $f_{T}(B_{1},\ldots,B_{T}) = T$. Alors $T \sim Geom(p)$. Bien sûr ici $T$ et $f_{T}$ ne sont pas indépendants puisqu'ils sont de même valeur. Donc, d'après la définition $5$ du chapitre $1$, cet algorithme est non-interruptible.\\
On considère ce problème du point de vue d'une personne souhaitant simuler ce problème. Peut-être que celle-ci, en mettant en place ce problème veut inconsciemment que si  l'algorithme prend trop de temps (ou de pas, par exemple, $5$ millions de pas), alors la personne fait preuve d'impatience et interrompt l'algorithme.\\
Cette décision est en fait une partie non-reconnue de l'algorithme, et signifie que l'on obtient pas d'échantillon suivant une loi $Geom(p)$ mais plutôt une loi $Geom(p)$ conditionné à être dans $\left\{ 1, \ldots, 5*10^{6} \right\}$. Dans notre cas, c'est peut-être anodin, sauf si $p$ est très petit. Le potentiel est là, et la volonté de la personne à vouloir arrêter l'algorithme fait que celui-ci n'est plus un algorithme de simulation exacte.\\
On donne à présent un exemple d'algorithme interruptible. Considérons la méthode de rejet permettant de tirer selon une mesure $\nu$ sur un ensemble $A$ sachant que l'on sait tirer selon la mesure $\nu$ sur $B$, et sachant que $A$ est contenu dans $B$.

\begin{lemme}
On suppose que $\nu$ est une mesure finie sur $B$, que $A \subset B$ et $\nu (A) >0$. Pour des variables iid$X_{1},X_{2},\ldots \sim \nu(B)$ et $T = \inf \left\{ t | X_{t} \in A \right\}$, on a que $T$ et $f_{T}(X_{1},\ldots,X_{T}) = X_{T}$ sont des variables aléatoires indépendantes. Et donc, la méthode de rejet est un algorithme interruptible.
\end{lemme}  

\begin{proof}
Soit $C$ un sous-ensemble mesurable de $A$ et $i \in \left\{ 1,2,\ldots \right\}$. Alors : 

\begin{align*}
\mathbb{P}(X_{T} \in C, T = i) &= \mathbb{P}(X_{1},\ldots,X_{i-1} \notin C, X_{i} \in C)\\
&= \left(1 - \frac{\nu(A)}{\nu(B)}\right)^{i-1} \left(\frac{\nu(C}{\nu(B)}\right)\\
&= \left(1 - \frac{\nu(A)}{\nu(B)}\right)^{i-1} \left(\frac{\nu(A)}{\nu(B)}\right)\left(\frac{\nu(C)}{\nu(A)}\right)\\
&= \mathbb{P}(X_{1},\ldots,X_{i-1} \notin C, X_{i} \in A)\mathbb{P}(X_{T} \in C)\\
&= \mathbb{P}(T = i) \mathbb{P}(X_{T} \in C)
\end{align*}

où $\mathbb{P}(X_{T} \in A) = \frac{\nu(A)}{\nu(B)}$ est le théorème $6$ du chapitre $2$. 

\end{proof}

L'avantage d'un algorithme interruptible est que l'utilisateur peut l'arrêter puis le relancer sans changer la sortie de l'algorithme. L'utilisateur n'a donc pas à se soucier de possibles limitations (connues ou non) sur le temps de calcul de l'algorithme.\\
Le problème est le suivant : en général, le CFTP est non-interruptible.\\
On rappelle d'abord que le CFTP décrit la distribution ciblée $\pi$ comme un mélange de deux autres distributions. Soit $A$ l'évènement tel que si $U \in A$, alors $\phi(x,U) = \left\{y\right\} ~ \forall x \in \Omega$. Alors, pour $Y \sim \pi$ et pour tout ensemble mesurable $C$, 

$$ \mathbb{P}(Y \in C) = \mathbb{P}(Y \in C | A)\mathbb{P}(A) + \mathbb{P}(Y \in C | A^{C}) \mathbb{P}(A^{C})$$

Lorsque que $A$ se réalise, $\phi(x,U) = \left\{y\right\}$, et on rend $y$. Lorsque $A^{C}$ se réalise, on utilise une récursion pour tirer $X \sim \pi$ puis $\phi(X,U)$ donne $[Y | A^{C}]$.\\
On suppose à présent que l'utilisateur n'a pas le temps d'effectuer la récursion. Alors le résultat n'est pas $Y \sim \pi$ mais plutôt $[Y | A]$.
Comme exemple, on considère le problème de tirer uniformément sur $\left\{1,2,3\right\}$ en utilisant la fonction de mise à jour de la chaîne de Markov suivante : 

$$ \phi_{1}(x,U) = x + \mathbf{1}(x < 3 , U > 1/2) - \mathbf{1}(x > 1, U \leq 1/2)$$

On suppose que $\phi_{2}(x,U_{1},U_{2}) = \phi_{1}(\phi_{1}(x,U_{1}),U_{2})$,ie, $\phi_{2}$ effectue deux pas dans la chaîne de Markov. Dans quel cas avons nous $\phi_{2}(\Omega,U_{1},U_{2}) = \left\{y\right\}$? Seulement dans le cas où $U_{1}$ et $U_{2}$ tombent tout deux dans $(1/2,1]$ ou $[0,1/2]$. Dans le premier cas $\phi_{2}(\Omega,U_{1},U_{2}) = \left\{3\right\}$ et dans le second $\phi_{2}(\Omega,U_{1},U_{2}) = \left\{1\right\}$. En aucun cas avons nous $\phi_{2}(\Omega,U_{1},U_{2}) = \left\{2\right\}$ donc le résultat ne peut-être uniforme sur $\left\{1,2,3\right\}$.\\
Des raisons connues (temps de calcul, temps autorisé par l'utilisateur) et des raisons inconnues (coupure de courant) signifient qu'en somme, chaque simulation est lancée avec une borne supérieure aléatoire sur le temps d'arrêt de l'algorithme. Si il y a une limite sur le temps pour lequel l'algorithme peut-être lancé, celui-ci cesse d'être un algorithme de simulation exacte.

\subsubsection{Lecture double}

L'autre problème du CFTP provient de sa structure. Lorsque les variables aléatoires $U$ sont générées, si la récursion s'opère, alors ces variables $u$ doivent être utilisées à nouveau. Donc, elle doivent en théorie être stockées, ce qui peut imposer un problème sur la mémoire. \\
Il y a deux moyens de pallier à ce défaut. Premièrement, on peut simplement ne garder en mémoire que la "seed" utilisée pour générer les choix alétoires.\\
De plus, il existe une variante du CFTP telle que les choix aléatoires n'ont pas à être stockés. Cette variante s'appelle le CFTP à lecture unique, que le livre d'Huber (Chapitre 5) aborde.





\end{document}